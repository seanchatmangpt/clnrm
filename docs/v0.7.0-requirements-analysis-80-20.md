# CLNRM v0.7.0 DX Features - 80/20 Requirements Analysis

**Date**: 2025-10-16
**Analyst**: Requirements Analyst (Research Agent)
**Focus**: Developer Experience (DX) - 80/20 Prioritization
**Status**: COMPREHENSIVE ANALYSIS COMPLETE

---

## Executive Summary

This analysis applies 80/20 prioritization to v0.7.0 DX features, identifying the 20% of features that will deliver 80% of developer productivity value. Based on analysis of the v0.7.0 architecture document and current codebase state, we've identified **3 critical HIGH-VALUE features** that should be implemented first.

**Key Finding**: The v0.7.0 architecture document proposes 5 features, but **dev --watch** and **macro pack** together deliver ~70% of the DX value while requiring only ~40% of the implementation effort.

---

## 1. Current State Assessment

### 1.1 v0.6.0 Foundation (Already Complete)

**Tera Templating System**:
- ✅ Template rendering engine (`TemplateRenderer`)
- ✅ Custom functions: `env()`, `now_rfc3339()`, `sha256()`, `toml_encode()`
- ✅ Template namespaces: `vars.*`, `matrix.*`, `otel.*`
- ✅ Deterministic rendering with seed/clock freezing
- ✅ Comprehensive documentation (3 architecture docs)

**OTEL Validation Framework**:
- ✅ Multi-dimensional validators (temporal, status, count, window, graph, hermeticity)
- ✅ False positive elimination (audit complete)
- ✅ Multi-format reporting (JSON, JUnit XML, SHA-256 digests)

**CLI Infrastructure**:
- ✅ Clap-based command parser
- ✅ 10 existing commands (init, run, validate, plugins, self-test, template, services, report, health, marketplace)
- ✅ Watch mode stub in `run.rs` (basic file watching exists)

### 1.2 v0.7.0 CLI Structure (Partially Implemented)

**In types.rs** (modified today):
```rust
/// Development mode with file watching (v0.7.0)
Dev { paths, debounce_ms, clear }

/// Dry-run validation without execution (v0.7.0)
DryRun { files, verbose }

/// Format Tera templates (v0.7.0)
Fmt { files, check, verify }

/// Lint TOML test configurations (v0.7.0)
Lint { files, format, deny_warnings }

/// Diff OpenTelemetry traces (v0.7.0)
Diff { baseline, current, format, only_changes }
```

**Status**: CLI types defined, but implementations NOT started (v0_7_0/ directory is empty).

---

## 2. Critical User Workflows

### Workflow 1: First-Time User (CRITICAL)
**Current Experience** (v0.6.0):
```bash
$ clnrm init                    # ~2s
$ vim tests/basic.clnrm.toml   # Manual edit
$ clnrm run                     # ~15-30s (container spin-up)
$ vim tests/basic.clnrm.toml   # Fix errors
$ clnrm run                     # ~15-30s again
```
**Pain**: 15-30s feedback loop kills productivity.

**Desired Experience** (v0.7.0 with dev --watch):
```bash
$ clnrm init                    # ~2s
$ clnrm dev --watch            # Starts watching
  [Auto-detects test files]
  [Runs tests immediately]
  [Shows results in <3s]
$ vim tests/basic.clnrm.toml   # Save triggers auto-rerun
  [Results appear in <3s]      # ✅ Instant feedback!
```
**Value**: 5-10x faster iteration, instant feedback, reduced context switching.

### Workflow 2: Debug Loop (CRITICAL)
**Current Experience**:
```bash
$ clnrm run tests/api.toml      # Test fails
$ vim tests/api.toml            # Fix attempt
$ clnrm run tests/api.toml      # Test fails again
$ vim tests/api.toml            # Another fix
$ clnrm run tests/api.toml      # Still fails
```
**Pain**: Manual command repetition, no incremental feedback.

**Desired Experience** (with dev --watch):
```bash
$ clnrm dev --watch tests/
  [Runs tests, shows failure]
$ vim tests/api.toml            # Save auto-triggers rerun
  [See new failure in <3s]
$ vim tests/api.toml            # Another save
  [See success in <3s] ✅
```
**Value**: Zero manual commands, focus on fixing not running.

### Workflow 3: Template Authoring (HIGH VALUE)
**Current Experience**:
```bash
$ vim test.clnrm.toml.tera
  # Write repetitive span validation blocks (10+ lines each)
  [[expect.span]]
  name = "api.request"
  kind = "server"
  attrs.all = { "service.name" = "api" }

  [[expect.span]]
  name = "api.response"
  kind = "server"
  attrs.all = { "service.name" = "api" }
  # Repeat 10 times... tedious!
```
**Pain**: Massive boilerplate, error-prone, hard to maintain.

**Desired Experience** (with macro pack):
```bash
$ vim test.clnrm.toml.tera
  {% import "_macros.toml.tera" as m %}

  # One-liner replaces 4 lines!
  {{ m::span("api.request", "server", {"service.name": "api"}) }}
  {{ m::span("api.response", "server", {"service.name": "api"}) }}

  # Service lifecycle in 1 line!
  {{ m::lifecycle("database") }}
```
**Value**: 70-80% less boilerplate, faster authoring, fewer errors.

### Workflow 4: Validation Without Execution (MEDIUM VALUE)
**Current Experience**:
```bash
$ vim tests/complex.toml        # Make changes
$ clnrm run tests/complex.toml  # Spins up containers (~15-30s)
  Error: Invalid TOML - typo in line 42
$ vim tests/complex.toml        # Fix typo
$ clnrm run tests/complex.toml  # Spins up containers AGAIN (~15-30s)
  Error: Missing service reference
```
**Pain**: Waiting for containers just to catch TOML errors.

**Desired Experience** (with dry-run):
```bash
$ vim tests/complex.toml        # Make changes
$ clnrm dry-run tests/complex.toml  # <1s validation
  Error: Invalid TOML - typo in line 42
  Error: Missing service reference at line 67
$ vim tests/complex.toml        # Fix both errors
$ clnrm dry-run tests/complex.toml  # <1s validation
  ✅ All validations passed
$ clnrm run tests/complex.toml  # Now actually run (confident it will work)
```
**Value**: 10-30x faster error detection, batch error fixes.

### Workflow 5: Formatting (LOWER PRIORITY)
**Current Experience**:
```toml
# Inconsistent formatting across files
[services.db]
type="generic_container"
image =  "postgres:15"

[services.api]
  type = "generic_container"
  image="nginx:alpine"
```
**Pain**: Hard to review, team inconsistency.

**Desired Experience** (with fmt):
```bash
$ clnrm fmt tests/
  Formatted: tests/api.toml
  Formatted: tests/db.toml
  2 files reformatted
```
**Value**: Consistent style, easier code review (but doesn't block development).

---

## 3. 80/20 Feature Prioritization Matrix

| Feature | DX Value | Implementation Effort | Value/Effort Ratio | Priority | Rationale |
|---------|----------|----------------------|-------------------|----------|-----------|
| **dev --watch** | 🔥 90% | Medium (2-3 days) | **45** | **HIGH** | Instant feedback loop, enables rapid iteration, removes manual command repetition |
| **Macro pack** | 🔥 80% | Low (1-2 days) | **40-80** | **HIGH** | Eliminates 70% of boilerplate, pairs perfectly with watch mode |
| **dry-run** | 🟡 60% | Low (1 day) | **60** | **MEDIUM** | Fast validation, but watch mode reduces need (auto-validation) |
| **fmt** | 🟢 30% | Medium (2-3 days) | **10-15** | **LOW** | Nice for consistency, but doesn't improve iteration speed |
| **Change detection** | 🟢 40% | Medium (2 days) | **20** | **MEDIUM** | Incremental runs, but watch mode provides similar benefit |

**80/20 Insight**:
- **dev --watch + macro pack** = 85% of DX value with only 35-40% of effort
- **dry-run** adds another 10% value for 15% more effort
- **fmt + change detection** = remaining 5% value but require 35% more effort

**Recommendation**: Focus on the **top 3 features** (dev, macro, dry-run) for v0.7.0 MVP.

---

## 4. Feature Breakdown & Acceptance Criteria

### 4.1 Feature: dev --watch (CRITICAL PATH - IMPLEMENT FIRST)

**User Story**:
> As a test author, I want my tests to auto-rerun on file save so that I get instant feedback without manual command execution.

**Acceptance Criteria**:
- [ ] `clnrm dev --watch` starts file watcher on current directory
- [ ] `clnrm dev --watch tests/` watches specific directory
- [ ] Debouncing prevents multiple runs from rapid saves (default 300ms)
- [ ] Terminal clears and shows fresh output on each run (with `--clear` flag)
- [ ] File changes trigger: render → validate → run → report pipeline
- [ ] Ctrl+C exits cleanly without hanging
- [ ] Errors in tests don't crash the watcher
- [ ] Watch latency <3s (p95) from save to results displayed

**Implementation Dependencies**:
- ✅ `notify = "6.0"` already in workspace dependencies
- ✅ Template rendering infrastructure exists (`TemplateRenderer`)
- ✅ Test execution pipeline exists (`run_tests()`)
- ✅ Watch stub exists in `run.rs::watch_and_run()`

**Implementation Plan**:
1. Create `crates/clnrm-core/src/cli/commands/v0_7_0/dev.rs`
2. Reuse existing `watch_and_run()` from `run.rs`
3. Add debouncing logic (use `tokio::time::sleep`)
4. Add terminal clearing (use `clearscreen` crate or ANSI codes)
5. Hook into CLI dispatcher in `main.rs`
6. Write integration test: save file → verify auto-rerun

**Estimated Effort**: 2-3 days
- Day 1: Core watcher implementation
- Day 2: Debouncing, terminal clearing, error handling
- Day 3: Integration tests, performance tuning

**Complexity**: Medium
- File watching: Low (library handles it)
- Debouncing: Low (simple delay)
- Pipeline integration: Low (reuse existing)
- Error recovery: Medium (need graceful handling)

**Risks**:
- ⚠️ Watch events on network filesystems (WSL, Docker mounts) - may need polling fallback
- ⚠️ Large directories could generate too many events - need filtering
- ⚠️ Infinite loops if tests write to watched directories - need exclusion patterns

**Mitigation**:
- Document network filesystem limitations
- Filter to only `*.toml`, `*.toml.tera`, `*.tera` files
- Exclude `target/`, `.git/`, `reports/` directories

---

### 4.2 Feature: Macro Pack (HIGH VALUE - IMPLEMENT SECOND)

**User Story**:
> As a test author, I want reusable macros for common OTEL validation patterns so that I can write tests 5x faster with less boilerplate.

**Acceptance Criteria**:
- [ ] `_macros.toml.tera` file ships with clnrm installation
- [ ] Installed to `~/.clnrm/templates/_macros.toml.tera` on `clnrm init`
- [ ] Users can import with `{% import "_macros.toml.tera" as m %}`
- [ ] Core macros implemented:
  - [ ] `m::span(name, kind, attrs)` - Generate span expectation
  - [ ] `m::lifecycle(service)` - Generate start/exec/stop spans
  - [ ] `m::edges(parent_child_pairs)` - Generate graph edges
  - [ ] `m::window(start, end)` - Generate time window constraint
  - [ ] `m::count(kind, min, max)` - Generate count constraint
- [ ] Macros render to valid TOML
- [ ] Documentation shows all macro signatures
- [ ] Examples demonstrate common patterns

**Implementation Dependencies**:
- ✅ Tera engine supports macros (built-in feature)
- ✅ Template rendering pipeline exists
- ⚠️ Need template search path configuration (add `~/.clnrm/templates/` to Tera search)

**Implementation Plan**:
1. Create `templates/_macros.toml.tera` in project root
2. Implement 5 core macros (span, lifecycle, edges, window, count)
3. Update `TemplateRenderer` to add `~/.clnrm/templates/` to search path
4. Update `init_project()` to copy `_macros.toml.tera` to `~/.clnrm/templates/`
5. Write integration tests: import macros, verify rendering
6. Document in `docs/TERA_TEMPLATE_GUIDE.md`

**Macro Specifications**:

**1. `span(name, kind, attrs={})`**
```tera
{% macro span(name, kind, attrs={}) %}
[[expect.span]]
name = "{{ name }}"
kind = "{{ kind }}"
{% if attrs %}
attrs.all = { {% for k, v in attrs %}"{{ k }}" = "{{ v }}"{{ ", " if not loop.last }}{% endfor %} }
{% endif %}
{% endmacro %}
```

**2. `lifecycle(service)`**
```tera
{% macro lifecycle(service) %}
{{ self::span(service ~ ".start", "internal") }}
{{ self::span(service ~ ".exec", "internal") }}
{{ self::span(service ~ ".stop", "internal") }}

[expect.order]
must_precede = [
  ["{{ service }}.start", "{{ service }}.exec"],
  ["{{ service }}.exec", "{{ service }}.stop"]
]
{% endmacro %}
```

**3. `edges(pairs)`**
```tera
{% macro edges(pairs) %}
[expect.graph]
must_include = [
{% for parent, child in pairs %}
  ["{{ parent }}", "{{ child }}"]{{ ", " if not loop.last }}
{% endfor %}
]
{% endmacro %}
```

**4. `window(start, end)`**
```tera
{% macro window(start, end) %}
[expect.window]
"{{ start }}" = { contains = ["{{ end }}"] }
{% endmacro %}
```

**5. `count(kind, min, max=none)`**
```tera
{% macro count(kind, min, max=none) %}
[expect.count]
by_kind.{{ kind }} = { min = {{ min }}{% if max %}, max = {{ max }}{% endif %} }
{% endmacro %}
```

**Usage Example**:
```toml
{% import "_macros.toml.tera" as m %}

[meta]
name = "api_test"
version = "0.7.0"

# Old way: 4 lines per span
[[expect.span]]
name = "api.request"
kind = "server"
attrs.all = { "http.method" = "GET" }

# New way: 1 line per span!
{{ m::span("api.request", "server", {"http.method": "GET"}) }}

# Service lifecycle in 1 line!
{{ m::lifecycle("database") }}

# Graph edges in 1 block!
{{ m::edges([("api.request", "db.query"), ("db.query", "db.result")]) }}
```

**Estimated Effort**: 1-2 days
- Day 1: Implement 5 macros, template path configuration
- Day 2: Integration tests, documentation, examples

**Complexity**: Low
- Macro syntax: Low (Tera built-in)
- Template path: Low (Tera configuration)
- Testing: Low (render and verify TOML)

**Risks**:
- ⚠️ Macro namespace collisions if users define same names
- ⚠️ Tera import paths may differ on Windows/Linux

**Mitigation**:
- Use `m::` prefix consistently to avoid collisions
- Test on multiple platforms
- Document import path resolution

---

### 4.3 Feature: dry-run (MEDIUM PRIORITY - IMPLEMENT THIRD)

**User Story**:
> As a test author, I want to validate my TOML configuration without spinning up containers so that I can catch errors in <1s instead of 15-30s.

**Acceptance Criteria**:
- [ ] `clnrm dry-run tests/*.toml` validates all files
- [ ] Validation completes in <1s for 10 files
- [ ] Checks all required blocks present ([meta], [otel], [[scenario]], [[expect.span]])
- [ ] Detects orphan service references (service used but not defined)
- [ ] Validates duration constraints (min < max)
- [ ] Validates glob patterns compile
- [ ] Validates temporal ordering is acyclic
- [ ] Reports all errors with file:line:column
- [ ] Exit code 0 if valid, 1 if errors
- [ ] `--verbose` flag shows detailed validation steps

**Validation Checks** (prioritized by 80/20):

**HIGH PRIORITY** (catch 80% of errors):
1. ✅ TOML syntax valid (parse succeeds)
2. ✅ Required blocks present ([meta], [services], [[scenario]])
3. ✅ Service references valid (no orphans)
4. ✅ Span names not empty
5. ✅ Duration min < max constraints

**MEDIUM PRIORITY** (catch next 15% of errors):
6. ✅ Glob patterns compile (`glob::Pattern::new()`)
7. ✅ Temporal ordering acyclic (no circular dependencies)
8. ✅ Attribute keys valid (no duplicates)

**LOW PRIORITY** (catch remaining 5%):
9. ⚠️ Image names valid Docker references (optional, may require network)
10. ⚠️ Environment variable names valid (optional, pedantic)

**Implementation Dependencies**:
- ✅ Config parsing exists (`config::load_config_from_file()`)
- ✅ Template rendering exists (`TemplateRenderer`)
- ⚠️ Need shape validator (`validation::shape` module)

**Implementation Plan**:
1. Create `crates/clnrm-core/src/cli/commands/v0_7_0/dry_run.rs`
2. Create `crates/clnrm-core/src/validation/shape.rs` (shape validator)
3. Implement validation checks (start with high priority only)
4. Return structured validation report
5. Format errors with file:line:column
6. Hook into CLI dispatcher
7. Write integration tests

**Validation Flow**:
```
1. Find files (*.toml, *.toml.tera)
   ↓
2. Render templates (if .tera)
   ↓
3. Parse TOML → Result<Config>
   ↓
4. Validate structure:
   - Required blocks?
   - Service references valid?
   - Duration constraints?
   - Temporal ordering acyclic?
   ↓
5. Report errors or success
```

**Error Format**:
```
tests/api.toml:42:5 - error: Missing required block [meta]
tests/db.toml:67:12 - error: Service 'postgres' referenced but not defined
tests/complex.toml:89:3 - error: Duration constraint invalid: min (100) > max (50)

3 errors found in 3 files
```

**Estimated Effort**: 1 day
- Morning: Shape validator implementation (4-6 hours)
- Afternoon: CLI integration, error formatting (2-4 hours)

**Complexity**: Low-Medium
- TOML parsing: Low (already exists)
- Validation logic: Medium (graph traversal for cycles)
- Error reporting: Low (string formatting)

**Risks**:
- ⚠️ False positives if validation too strict
- ⚠️ False negatives if validation too lenient
- ⚠️ Performance on 100+ file projects

**Mitigation**:
- Start with high-priority checks only (minimal false positives)
- Add `--strict` flag for pedantic checks
- Benchmark on large projects, optimize if needed

---

### 4.4 Feature: fmt (LOWER PRIORITY - DEFER TO v0.7.1)

**User Story**:
> As a test author, I want deterministic TOML formatting so that code reviews focus on logic not style.

**Acceptance Criteria**:
- [ ] `clnrm fmt tests/` formats all TOML files
- [ ] Idempotent (formatting twice produces same output)
- [ ] Keys sorted alphabetically within sections
- [ ] 2-space indentation
- [ ] No trailing whitespace
- [ ] `--check` flag for CI (exit 1 if unformatted)
- [ ] Preserves comments (if possible)

**Why Lower Priority**:
- ❌ Doesn't improve iteration speed
- ❌ Doesn't unblock users
- ✅ Nice for consistency, but not critical for MVP

**Estimated Effort**: 2-3 days
**Complexity**: Medium (requires `toml_edit` crate for comment preservation)

**Defer Decision**: Implement in v0.7.1 after user feedback on core features.

---

### 4.5 Feature: Change Detection (LOWER PRIORITY - DEFER)

**User Story**:
> As a test author, I want only changed tests to rerun so that iteration is 10x faster.

**Acceptance Criteria**:
- [ ] Calculates SHA-256 of rendered TOML
- [ ] Stores hashes in `.clnrm/cache/hashes.json`
- [ ] Skips tests if hash unchanged
- [ ] `--force` flag bypasses cache

**Why Lower Priority**:
- ❌ `dev --watch` already provides fast iteration
- ❌ Incremental benefit only on large test suites (100+ tests)
- ✅ Nice optimization, but not MVP-critical

**Estimated Effort**: 2 days
**Complexity**: Medium (cache management, invalidation logic)

**Defer Decision**: Implement in v0.7.1+ if users request it.

---

## 5. Dependency Mapping

### Dependency Graph

```
v0.6.0 Foundation (✅ COMPLETE)
├── TemplateRenderer
├── Tera Functions (env, now_rfc3339, sha256, toml_encode)
├── OTEL Validators (temporal, status, count, window, graph)
└── Multi-format Reporting (JSON, JUnit, SHA-256)

v0.7.0 Critical Path
├── dev --watch (Priority 1)
│   ├── Depends: notify crate ✅
│   ├── Depends: TemplateRenderer ✅
│   ├── Depends: run_tests() ✅
│   └── Blocks: Nothing (standalone)
│
├── Macro Pack (Priority 2)
│   ├── Depends: Tera macros ✅
│   ├── Depends: Template search path (NEW)
│   ├── Depends: init_project() ✅
│   └── Blocks: Nothing (enhances watch experience)
│
└── dry-run (Priority 3)
    ├── Depends: Config parser ✅
    ├── Depends: Shape validator (NEW)
    └── Blocks: Nothing (standalone)

v0.7.0 Deferred
├── fmt (v0.7.1+)
│   ├── Depends: toml_edit crate (NEW)
│   └── Complexity: Medium
│
└── Change detection (v0.7.1+)
    ├── Depends: SHA-256 hashing ✅
    ├── Depends: Cache management (NEW)
    └── Complexity: Medium
```

### New Dependencies Required

| Dependency | Purpose | Version | License | Size | Risk |
|-----------|---------|---------|---------|------|------|
| ✅ `notify = "6.0"` | File watching | 6.0 | CC0-1.0 | ~200KB | Low (already in workspace) |
| ⚠️ `toml_edit = "0.22"` | Comment-preserving formatter | 0.22 | MIT/Apache | ~150KB | Low (only for fmt, deferred) |

**Note**: No new critical dependencies for v0.7.0 MVP (dev, macro, dry-run)!

---

## 6. Implementation Roadmap (Critical Path)

### Phase 1: MVP (2 weeks) - 80% of value

**Week 1: Core DX Loop**
- **Days 1-3**: Implement `dev --watch` command
  - Day 1: Core watcher, debouncing
  - Day 2: Error handling, terminal clearing
  - Day 3: Integration tests, performance tuning
  - **Blocker Removal**: Manual command repetition
  - **Value Unlocked**: 50% of total DX improvement

- **Days 4-5**: Implement macro pack
  - Day 4: Core macros, template path config
  - Day 5: Integration tests, documentation
  - **Blocker Removal**: Boilerplate authoring
  - **Value Unlocked**: 30% of total DX improvement

**Week 2: Validation**
- **Day 6**: Implement `dry-run` command
  - Morning: Shape validator
  - Afternoon: CLI integration, error formatting
  - **Blocker Removal**: Slow TOML validation
  - **Value Unlocked**: 10% of total DX improvement

- **Days 7-10**: Testing, documentation, polish
  - Day 7: Integration tests (all features)
  - Day 8: Performance benchmarks
  - Day 9: Documentation (user guides)
  - Day 10: Examples, release preparation

**Total MVP Timeline**: 10 days (2 weeks)

### Phase 2: Polish (Optional, post-MVP)

**Week 3+: Enhancements (20% of value)**
- `fmt` command (2-3 days)
- Change detection (2 days)
- Additional macros (1 day)
- Performance optimizations (2 days)

**Total Phase 2**: 1-2 weeks

---

## 7. Risk Assessment

### High-Risk Areas

**1. Watch Mode Performance on Large Directories**
- **Risk**: 100+ file projects may generate too many watch events
- **Mitigation**: Filter to `*.toml`, `*.toml.tera` only
- **Fallback**: Add `--include` / `--exclude` flags

**2. Cross-Platform File Watching**
- **Risk**: Different behavior on Linux/macOS/Windows/WSL
- **Mitigation**: Test on all platforms before release
- **Fallback**: Polling mode for problematic environments

**3. Template Import Path Resolution**
- **Risk**: `~/.clnrm/templates/` may not resolve correctly on Windows
- **Mitigation**: Use `dirs` crate for cross-platform paths
- **Fallback**: Document manual template placement

### Medium-Risk Areas

**4. Validation False Positives**
- **Risk**: Shape validator too strict, rejects valid configs
- **Mitigation**: Start with high-priority checks only
- **Fallback**: Add `--strict` flag for pedantic mode

**5. Macro Namespace Collisions**
- **Risk**: User-defined macros conflict with shipped macros
- **Mitigation**: Use `m::` prefix consistently
- **Fallback**: Document macro naming conventions

### Low-Risk Areas

**6. Debouncing Tuning**
- **Risk**: 300ms default may be too fast/slow for some users
- **Mitigation**: Make configurable via `--debounce-ms` flag
- **Impact**: Low (easy to adjust)

---

## 8. Success Metrics

### Developer Productivity Metrics (80/20 Focus)

**PRIMARY METRIC - Iteration Speed**:
- **Current (v0.6.0)**: 15-30s per edit-run cycle
- **Target (v0.7.0)**: <3s per edit-run cycle (p95)
- **Measurement**: Time from file save to test results displayed

**SECONDARY METRIC - Boilerplate Reduction**:
- **Current (v0.6.0)**: 10 lines TOML per span expectation
- **Target (v0.7.0)**: 1 line TOML per span expectation (with macros)
- **Measurement**: Lines of code in typical test files

**TERTIARY METRIC - Error Detection Time**:
- **Current (v0.6.0)**: 15-30s to detect TOML errors (requires container run)
- **Target (v0.7.0)**: <1s to detect TOML errors (with dry-run)
- **Measurement**: Time from file save to error report

### Adoption Metrics

**Feature Usage** (track via telemetry opt-in):
- `dev --watch` sessions per user per week
- Macro import frequency in template files
- `dry-run` invocations per user per week

**User Satisfaction** (post-release survey):
- "How much faster are you iterating on tests?" (1-10 scale)
- "How much has boilerplate reduced?" (1-10 scale)
- "Would you recommend v0.7.0 to a colleague?" (NPS score)

---

## 9. Feature Comparison: v0.6.0 → v0.7.0

| Capability | v0.6.0 | v0.7.0 MVP | Improvement |
|-----------|--------|-----------|-------------|
| **Edit-run latency** | 15-30s | <3s | **5-10x faster** |
| **Boilerplate per span** | 10 lines | 1 line | **10x less code** |
| **Error detection** | 15-30s (container run) | <1s (dry-run) | **15-30x faster** |
| **Auto-rerun on save** | ❌ Manual | ✅ Automatic | **New capability** |
| **Template macros** | ❌ None | ✅ 5 core macros | **New capability** |
| **TOML validation** | ❌ Runtime only | ✅ Pre-flight | **New capability** |
| **TOML formatting** | ❌ Manual | ⏸️ Deferred | Not in MVP |
| **Incremental runs** | ❌ Full rerun | ⏸️ Deferred | Not in MVP |

**Total Value Increase**: ~**8x productivity improvement** for typical workflows.

---

## 10. Dependencies to Add/Remove

### Add (for MVP)
**NONE** - All dependencies already in workspace! ✅

### Add (for Phase 2, deferred)
- `toml_edit = "0.22"` (for fmt command)

### Update
- ✅ `notify = "6.0"` (already in workspace, verify version)

---

## 11. Backward Compatibility

**Breaking Changes**: NONE ✅

**New Features (Opt-In)**:
- `clnrm dev --watch` (new command)
- `clnrm dry-run` (new command)
- `_macros.toml.tera` (opt-in import)

**Existing Workflows Unchanged**:
- ✅ `clnrm init` works exactly as before
- ✅ `clnrm run` works exactly as before
- ✅ All `.toml` and `.toml.tera` files work unchanged
- ✅ All CLI flags remain compatible

---

## 12. Testing Strategy

### Unit Tests (per feature)

**dev --watch**:
- File change detection triggers rerun
- Debouncing prevents duplicate runs
- Error in test doesn't crash watcher
- Ctrl+C exits cleanly

**Macro pack**:
- Each macro renders valid TOML
- Macros composable (nested calls)
- Import statement resolves correctly
- Template search path includes `~/.clnrm/templates/`

**dry-run**:
- Valid configs pass all checks
- Missing required blocks detected
- Orphan service references detected
- Invalid duration constraints detected
- Acyclic temporal ordering verified

### Integration Tests

**E2E Workflow Tests**:
1. `clnrm init` → `clnrm dev --watch` → save file → verify auto-rerun
2. Import macros → render template → verify TOML structure
3. Invalid TOML → `clnrm dry-run` → verify error report

### Performance Benchmarks

**Watch Latency**:
- p50: <1.5s from save to results
- p95: <3s from save to results
- p99: <5s from save to results

**Dry-Run Speed**:
- 10 files: <1s total
- 100 files: <5s total

### Manual Testing

**Cross-Platform**:
- macOS (Intel + Apple Silicon)
- Linux (Ubuntu 22.04, Fedora 39)
- Windows 11 (native + WSL2)

**Network Filesystems**:
- Docker bind mounts
- WSL2 mounted drives
- NFS/SMB shares

---

## 13. Documentation Requirements

### User-Facing Docs

**New Guides** (required):
1. `/Users/sac/clnrm/docs/DEV_MODE_GUIDE.md`
   - Getting started with dev --watch
   - Debouncing configuration
   - Troubleshooting watch issues

2. `/Users/sac/clnrm/docs/MACRO_REFERENCE.md`
   - All 5 macro signatures
   - Usage examples
   - Macro authoring guide

3. `/Users/sac/clnrm/docs/VALIDATION_GUIDE.md`
   - dry-run command usage
   - Validation error explanations
   - CI/CD integration

**Updated Guides** (required):
1. `/Users/sac/clnrm/README.md`
   - Add v0.7.0 highlights
   - Update quick start with dev --watch

2. `/Users/sac/clnrm/docs/CLI_GUIDE.md`
   - Add dev, dry-run, fmt, lint commands

3. `/Users/sac/clnrm/docs/TERA_TEMPLATES.md`
   - Add macro pack documentation

### Developer-Facing Docs

**Architecture Docs** (existing):
- ✅ `/Users/sac/clnrm/docs/V0.7.0_ARCHITECTURE.md` (complete)

**Implementation Docs** (new):
1. `/Users/sac/clnrm/docs/architecture/watch-implementation.md`
   - File watching architecture
   - Debouncing logic
   - Performance considerations

2. `/Users/sac/clnrm/docs/architecture/macro-system.md`
   - Macro resolution
   - Template search paths
   - Macro authoring guidelines

---

## 14. Migration Guide

**From v0.6.0 to v0.7.0**:

### Breaking Changes
**NONE** ✅ - All existing workflows continue to work unchanged.

### New Capabilities (Opt-In)

**1. Enable Watch Mode**:
```bash
# Old way (v0.6.0)
clnrm run tests/

# New way (v0.7.0)
clnrm dev --watch tests/
```

**2. Use Macro Pack**:
```toml
# Add to top of .toml.tera file
{% import "_macros.toml.tera" as m %}

# Replace verbose blocks
{{ m::span("api.request", "server", {"http.method": "GET"}) }}
{{ m::lifecycle("database") }}
```

**3. Pre-Flight Validation**:
```bash
# Before running tests
clnrm dry-run tests/*.toml
```

### Recommended Workflow

**New User Onboarding** (v0.7.0):
```bash
# Step 1: Initialize
clnrm init

# Step 2: Start watch mode
clnrm dev --watch

# Step 3: Edit and save
vim tests/basic.clnrm.toml

# Results appear automatically in <3s!
```

---

## 15. Post-Release Validation

### Week 1: Critical Bug Fixes
- Monitor GitHub issues for watch mode failures
- Fix platform-specific file watching issues
- Address macro import path problems

### Week 2-4: User Feedback
- Conduct user surveys on iteration speed
- Collect macro usage patterns
- Identify most common dry-run errors

### Month 2: Enhancements
- Add requested macros to macro pack
- Tune debouncing defaults based on feedback
- Implement fmt/lint if high demand

---

## 16. Appendix: Detailed Workflows

### Appendix A: First-Time User Journey (with v0.7.0)

**Persona**: Junior developer, first time using CLNRM

**Workflow**:
```bash
# Terminal 1: Setup
$ git clone https://github.com/mycompany/api-tests
$ cd api-tests
$ clnrm init
✅ Project initialized
Generated: tests/basic.clnrm.toml
Generated: README.md

# Terminal 1: Start watch mode
$ clnrm dev --watch
🔍 Watching for changes in: /Users/dev/api-tests
📋 Found 1 test file
🚀 Running tests...
  ✅ basic_test (127ms)
✨ All tests passed!
👀 Press Ctrl+C to stop watching

# Terminal 2: Edit test
$ vim tests/basic.clnrm.toml
  # Change expected output
  expected_output_regex = "Hello.*" → "Goodbye.*"
  # Save file (:wq)

# Terminal 1: Auto-rerun
📝 File changed: tests/basic.clnrm.toml
🚀 Running tests...
  ❌ basic_test (134ms)
     Expected: "Goodbye.*"
     Got: "Hello from cleanroom!"

# Terminal 2: Fix test
$ vim tests/basic.clnrm.toml
  # Revert change
  expected_output_regex = "Goodbye.*" → "Hello.*"
  # Save file (:wq)

# Terminal 1: Auto-rerun again
📝 File changed: tests/basic.clnrm.toml
🚀 Running tests...
  ✅ basic_test (129ms)
✨ All tests passed!
```

**Time to First Green**: <60s (including git clone)
**Edit-Fix-Green Cycle**: <10s total
**Manual Commands**: 2 (init, dev --watch) - zero after that!

---

### Appendix B: Advanced User Journey (with macros)

**Persona**: Senior SRE, writing complex OTEL validation tests

**Workflow**:
```bash
# Create test with macros
$ vim tests/distributed-trace.clnrm.toml.tera
```

```toml
{% import "_macros.toml.tera" as m %}

[meta]
name = "distributed_api_trace"
version = "0.7.0"

[vars]
service = "api-gateway"

[services.gateway]
plugin = "generic_container"
image = "myapi:latest"

[services.backend]
plugin = "generic_container"
image = "backend:latest"

[[scenario]]
name = "distributed_call"
service = "gateway"
run = "curl http://localhost:8080/api/users"

# OLD WAY (40 lines of TOML):
# [[expect.span]]
# name = "gateway.request"
# kind = "server"
# attrs.all = { "service.name" = "api-gateway" }
#
# [[expect.span]]
# name = "gateway.backend_call"
# kind = "client"
# attrs.all = { "service.name" = "api-gateway" }
#
# ... 30 more lines ...

# NEW WAY (10 lines with macros):
{{ m::lifecycle("gateway") }}
{{ m::lifecycle("backend") }}

{{ m::edges([
  ("gateway.request", "gateway.backend_call"),
  ("gateway.backend_call", "backend.request"),
  ("backend.request", "backend.db_query")
]) }}

{{ m::window("gateway.request", "backend.request") }}

{{ m::count("server", 2, 4) }}
{{ m::count("client", 1, 3) }}
```

**Lines of Code**:
- Old way: 150+ lines
- New way: 35 lines
- **Reduction**: 77% less code!

**Time to Author**:
- Old way: 30-45 minutes
- New way: 5-10 minutes
- **Speedup**: 4-6x faster!

---

### Appendix C: CI/CD Integration (with dry-run)

**Use Case**: Pre-commit hook to validate TOML before pushing

**Git Hook** (`.git/hooks/pre-commit`):
```bash
#!/bin/bash

# Fast validation before commit
echo "Validating TOML files..."
clnrm dry-run tests/*.toml

if [ $? -ne 0 ]; then
  echo "❌ TOML validation failed. Fix errors before committing."
  exit 1
fi

echo "✅ TOML validation passed"
exit 0
```

**CI Pipeline** (`.github/workflows/test.yml`):
```yaml
name: Test

on: [push, pull_request]

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install clnrm
        run: cargo install clnrm

      - name: Validate TOML configs
        run: clnrm dry-run tests/*.toml

      - name: Run integration tests
        run: clnrm run tests/
```

**Value**:
- Pre-commit: Catch errors in <1s before push
- CI: Fail fast on invalid configs (no container spin-up)
- **Time Saved**: 5-10 minutes per failed CI run

---

## 17. Final Recommendations

### For Product Owner

**Recommended v0.7.0 Scope** (80/20 Focus):
1. ✅ `dev --watch` (2-3 days) - **50% of DX value**
2. ✅ Macro pack (1-2 days) - **30% of DX value**
3. ✅ `dry-run` (1 day) - **10% of DX value**
4. ⏸️ `fmt` (2-3 days) - **5% of DX value** - DEFER to v0.7.1
5. ⏸️ Change detection (2 days) - **5% of DX value** - DEFER to v0.7.1

**Total MVP Effort**: 4-6 days of implementation + 4-5 days testing/docs = **10 business days**

**Total Value**: **90% of DX improvement** with only **60% of planned effort**

**Release Confidence**: HIGH
- No new critical dependencies
- No breaking changes
- Reuses v0.6.0 foundation
- Clear user value

### For Engineering Team

**Critical Path Implementation Order**:
1. **Week 1, Days 1-3**: `dev --watch` (highest value, enables fast iteration)
2. **Week 1, Days 4-5**: Macro pack (pairs well with watch mode)
3. **Week 2, Day 1**: `dry-run` (completes validation story)
4. **Week 2, Days 2-5**: Testing, docs, polish

**Parallel Work Opportunities**:
- Docs can be written while features are being implemented
- Macro pack independent of watch mode (can parallelize)
- Integration tests can start on Day 4

**Risk Mitigation**:
- Test on all platforms early (Day 2 of watch implementation)
- Benchmark performance on large projects (100+ files)
- User beta test before official release

### For Documentation Team

**Documentation Priorities**:
1. **HIGH**: Dev mode guide (unblocks users)
2. **HIGH**: Macro reference (enables feature discovery)
3. **MEDIUM**: Validation guide (explains error messages)
4. **LOW**: Architecture docs (for contributors)

**Estimated Effort**: 3-4 days for all user-facing docs

---

## 18. Conclusion

**v0.7.0 DX Features deliver an 8-10x productivity improvement** by focusing on the 20% of features that provide 80% of developer value:

1. **dev --watch**: Eliminates manual command repetition, provides <3s feedback loop
2. **Macro pack**: Reduces boilerplate by 70-80%, speeds authoring 5x
3. **dry-run**: Catches errors in <1s instead of 15-30s

**Total Implementation**: 10 business days (2 weeks)
**Total Value**: 90% of planned DX improvement
**Risk**: Low (no new dependencies, no breaking changes)
**User Impact**: Transformative for daily workflows

**Deferred to v0.7.1+**:
- `fmt` command (5% value, 20% effort)
- Change detection (5% value, 15% effort)

This 80/20 approach ensures we deliver maximum user value in minimum time, with high confidence and low risk.

---

**Next Steps**: Present findings to Architecture Sub-Coordinator for implementation prioritization.
