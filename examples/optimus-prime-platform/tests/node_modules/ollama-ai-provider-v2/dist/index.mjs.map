{"version":3,"sources":["../src/ollama-provider.ts","../src/completion/ollama-completion-language-model.ts","../src/completion/ollama-error.ts","../src/adaptors/convert-to-ollama-completion-prompt.ts","../src/adaptors/map-ollama-finish-reason.ts","../src/common/get-response-metadata.ts","../src/embedding/ollama-embedding-model.ts","../src/responses/ollama-responses-language-model.ts","../src/responses/ollama-responses-request-builder.ts","../src/responses/convert-to-ollama-responses-messages.ts","../src/adaptors/convert-to-ollama-chat-messages.ts","../src/responses/ollama-responses-prepare-tools.ts","../src/ollama-chat-settings.ts","../src/responses/ollama-responses-processor.ts","../src/responses/ollama-responses-stream-processor.ts"],"sourcesContent":["import {\n  EmbeddingModelV2,\n  LanguageModelV2,\n  ProviderV2,\n  NoSuchModelError,\n} from '@ai-sdk/provider';\nimport {\n  FetchFunction,\n  withoutTrailingSlash,\n} from '@ai-sdk/provider-utils';\nimport { OllamaChatModelId, OllamaProviderOptions, ollamaProviderOptions } from './ollama-chat-settings';\nimport { OllamaCompletionLanguageModel } from './completion/ollama-completion-language-model';\nimport {\n  OllamaCompletionModelId,\n  OllamaCompletionSettings,\n} from './completion/ollama-completion-settings';\nimport { OllamaEmbeddingModel } from './embedding/ollama-embedding-model';\nimport {\n  OllamaEmbeddingModelId,\n  OllamaEmbeddingSettings,\n} from './embedding/ollama-embedding-settings';\nimport { OllamaResponsesLanguageModel } from './responses/ollama-responses-language-model';\n\nexport interface OllamaProvider extends ProviderV2 {\n  (modelId: OllamaChatModelId): LanguageModelV2;\n\n  /**\nCreates an Ollama model for text generation.\n   */\n  languageModel(modelId: OllamaChatModelId): LanguageModelV2;\n\n  /**\nCreates an Ollama chat model for text generation.\n   */\n  chat(\n    modelId: OllamaChatModelId,\n    settings?: OllamaProviderOptions,\n  ): LanguageModelV2;\n\n  /**\nCreates an Ollama completion model for text generation.\n   */\n  completion(\n    modelId: OllamaCompletionModelId,\n    settings?: OllamaCompletionSettings,\n  ): LanguageModelV2;\n\n  /**\nCreates a model for text embeddings.\n   */\n  embedding(\n    modelId: OllamaEmbeddingModelId,\n    settings?: OllamaEmbeddingSettings,\n  ): EmbeddingModelV2<string>;\n\n  /**\nCreates a model for text embeddings.\n\n@deprecated Use `textEmbeddingModel` instead.\n   */\n  textEmbedding(\n    modelId: OllamaEmbeddingModelId,\n    settings?: OllamaEmbeddingSettings,\n  ): EmbeddingModelV2<string>;\n\n  /**\nCreates a model for text embeddings.\n   */\n  textEmbeddingModel(\n    modelId: OllamaEmbeddingModelId,\n    settings?: OllamaEmbeddingSettings,\n  ): EmbeddingModelV2<string>;\n\n}\n\nexport interface OllamaProviderSettings {\n  /**\nBase URL for the Ollama API calls.\n     */\n  baseURL?: string;\n\n  /**\nOllama Organization.\n     */\n  organization?: string;\n\n  /**\nOllama project.\n     */\n  project?: string;\n\n  /**\nCustom headers to include in the requests.\n     */\n  headers?: Record<string, string>;\n\n  /**\nOllama compatibility mode. Should be set to `strict` when using the Ollama API,\nand `compatible` when using 3rd party providers. In `compatible` mode, newer\ninformation such as streamOptions are not being sent. Defaults to 'compatible'.\n   */\n  compatibility?: 'strict' | 'compatible';\n\n  /**\nProvider name. Overrides the `ollama` default name for 3rd party providers.\n   */\n  name?: string;\n\n  /**\nCustom fetch implementation. You can use it as a middleware to intercept requests,\nor to provide a custom fetch implementation for e.g. testing.\n    */\n  fetch?: FetchFunction;\n}\n\n/**\nCreate an Ollama provider instance.\n */\nexport function createOllama(\n  options: OllamaProviderSettings = {},\n): OllamaProvider {\n  const baseURL =\n    withoutTrailingSlash(options.baseURL) ?? 'http://127.0.0.1:11434/api';\n\n  const providerName = options.name ?? 'ollama';\n\n  const getHeaders = () => ({\n    'Ollama-Organization': options.organization,\n    'Ollama-Project': options.project,\n    ...options.headers,\n  });\n\n  const createCompletionModel = (\n    modelId: OllamaCompletionModelId,\n    settings: OllamaCompletionSettings = {},\n  ) =>\n    new OllamaCompletionLanguageModel(modelId, settings, {\n      provider: `${providerName}.completion`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n    });\n\n  const createEmbeddingModel = (\n    modelId: OllamaEmbeddingModelId,\n    settings: OllamaEmbeddingSettings = {},\n  ) =>\n    new OllamaEmbeddingModel(modelId, settings, {\n      provider: `${providerName}.embedding`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n    });\n\n  const createLanguageModel = (\n    modelId: OllamaChatModelId) => {\n    if (new.target) {\n      throw new Error(\n        'The Ollama model function cannot be called with the new keyword.',\n      );\n    }\n\n    return createResponsesModel(modelId);\n  };\n\n  const createResponsesModel = (modelId: OllamaChatModelId) => {\n    return new OllamaResponsesLanguageModel(modelId, {\n      provider: `${providerName}.responses`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n    });\n  };\n\n  const provider = function (modelId: OllamaChatModelId) {\n    return createLanguageModel(modelId);\n  };\n\n  provider.languageModel = createLanguageModel;\n  provider.chat = createLanguageModel;\n  provider.completion = createCompletionModel;\n  provider.embedding = createEmbeddingModel;\n  provider.textEmbedding = createEmbeddingModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.imageModel = (modelId: string) => {\n    throw new NoSuchModelError({\n      modelId,\n      modelType: 'imageModel',\n      message: 'Image generation is unsupported with Ollama',\n    });\n  };\n\n  return provider as OllamaProvider;\n}\n\n/**\nDefault Ollama provider instance.\n */\nexport const ollama = createOllama();\n\n","import {\n  combineHeaders,\n  createJsonResponseHandler,\n  createJsonStreamResponseHandler,\n  generateId,\n  parseProviderOptions,\n  ParseResult,\n  postJsonToApi,\n} from \"@ai-sdk/provider-utils\";\nimport { z } from \"zod/v4\";\nimport {\n  InvalidPromptError,\n  LanguageModelV2,\n  LanguageModelV2CallWarning,\n  LanguageModelV2Content,\n  LanguageModelV2FinishReason,\n  LanguageModelV2StreamPart,\n  LanguageModelV2Usage,\n  SharedV2ProviderMetadata,\n} from \"@ai-sdk/provider\";\nimport { OllamaConfig } from \"../common/ollama-config\";\nimport { ollamaFailedResponseHandler } from \"./ollama-error\";\nimport { convertToOllamaCompletionPrompt } from \"../adaptors/convert-to-ollama-completion-prompt\";\nimport { OllamaCompletionModelId, OllamaCompletionSettings } from \"./ollama-completion-settings\";\nimport { mapOllamaFinishReason } from \"../adaptors/map-ollama-finish-reason\";\nimport { getResponseMetadata } from \"../common/get-response-metadata\";\n\n// Completion-specific provider options schema\nconst ollamaCompletionProviderOptions = z.object({\n  think: z.boolean().optional(),\n  user: z.string().optional(),\n  suffix: z.string().optional(),\n  echo: z.boolean().optional(),\n});\n\ntype OllamaCompletionConfig = {\n  provider: string;\n  url: (options: { path: string; modelId: string }) => string;\n  headers: () => Record<string, string | undefined>;\n  fetch?: typeof fetch;\n};\n\nexport type OllamaCompletionProviderOptions = z.infer<typeof ollamaCompletionProviderOptions>;\n\nexport class OllamaCompletionLanguageModel implements LanguageModelV2 {\n  readonly specificationVersion = \"v2\";\n\n  readonly modelId: OllamaCompletionModelId;\n  readonly settings: OllamaCompletionSettings;\n\n  private readonly config: OllamaCompletionConfig;\n\n  constructor(\n    modelId: OllamaCompletionModelId,\n    settings: OllamaCompletionSettings,\n    config: OllamaCompletionConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  readonly supportedUrls: Record<string, RegExp[]> = {\n    // No URLs are supported for completion models.\n  };\n\n  private async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    tools,\n    toolChoice,\n    seed,\n    providerOptions,\n  }: Parameters<LanguageModelV2[\"doGenerate\"]>[0]) {\n    const warnings: LanguageModelV2CallWarning[] = [];\n\n    // Parse provider options\n    const ollamaOptions =\n      (await parseProviderOptions({\n        provider: \"ollama\",\n        providerOptions,\n        schema: ollamaCompletionProviderOptions,\n      })) ?? {};\n\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\",\n      });\n    }\n\n    if (tools?.length) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"tools\" });\n    }\n\n    if (toolChoice != null) {\n      warnings.push({ type: \"unsupported-setting\", setting: \"toolChoice\" });\n    }\n\n    if (responseFormat != null && responseFormat.type !== \"text\") {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format is not supported.\",\n      });\n    }\n\n    const { prompt: completionPrompt, stopSequences } =\n      convertToOllamaCompletionPrompt({ prompt });\n\n    const stop = [...(stopSequences ?? []), ...(userStopSequences ?? [])];\n\n    return {\n      args: {\n        // model id:\n        model: this.modelId,\n\n        // Ollama-supported settings:\n        user: ollamaOptions.user,\n        think: ollamaOptions.think,\n\n        // standardized settings:\n        max_tokens: maxOutputTokens,\n        temperature,\n        top_p: topP,\n        frequency_penalty: frequencyPenalty,\n        presence_penalty: presencePenalty,\n        stop,\n\n        // prompt:\n        prompt: completionPrompt,\n\n        // other settings:\n        suffix: ollamaOptions.suffix,\n        echo: ollamaOptions.echo,\n        stream: false, // always disabled for doGenerate\n      },\n      warnings,\n    };\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV2[\"doGenerate\"]>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2[\"doGenerate\"]>>> {\n    const { args: body, warnings } = await this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/generate\",\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: { ...body, stream: false },\n      failedResponseHandler: ollamaFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        baseOllamaResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = body;\n\n    const providerMetadata: SharedV2ProviderMetadata = { ollama: {} };\n\n    return {\n      content: [\n        {\n          type: \"text\",\n          text: response.response,\n        },\n      ],\n      usage: {\n        inputTokens: response.prompt_eval_count ?? undefined,\n        outputTokens: response.eval_count ?? undefined,\n        totalTokens: (response.prompt_eval_count ?? 0) + (response.eval_count ?? 0),\n      },\n      finishReason: mapOllamaFinishReason(\"stop\"),\n      request: { body: JSON.stringify(body) },\n      response: {\n        ...getResponseMetadata(response),\n        headers: responseHeaders,\n        body: rawResponse,\n      },\n      warnings,\n      providerMetadata,\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV2[\"doStream\"]>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2[\"doStream\"]>>> {\n    const { args, warnings } = await this.getArgs(options);\n\n    const body = {\n      ...args,\n      stream: true,\n    };\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/generate\",\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: ollamaFailedResponseHandler,\n      successfulResponseHandler: createJsonStreamResponseHandler(\n        baseOllamaResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = args;\n\n    let finishReason: LanguageModelV2FinishReason = \"unknown\";\n    let usage: LanguageModelV2Usage = {\n      inputTokens: undefined,\n      outputTokens: undefined,\n      totalTokens: undefined,\n    };\n    let isFirstChunk = true;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof baseOllamaResponseSchema>>,\n          LanguageModelV2StreamPart\n        >({\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: (chunk as any).error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            // handle error chunks:\n            if (\"error\" in value) {\n              finishReason = \"error\";\n              controller.enqueue({ type: \"error\", error: value.error });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: \"response-metadata\",\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.done) {\n              finishReason = mapOllamaFinishReason(\"stop\");\n            }\n\n            if (value.response != null) {\n              controller.enqueue({\n                type: \"text-delta\",\n                id: \"0\",\n                delta: value.response,\n              });\n            }\n          },\n\n          flush(controller) {\n            controller.enqueue({\n              type: \"finish\",\n              finishReason,\n              usage,\n            });\n          },\n        }),\n      ),\n      request: { body: JSON.stringify(body) },\n      response: { headers: responseHeaders },\n    };\n  }\n}\n\nconst baseOllamaResponseSchema = z.object({\n  model: z.string(),\n  created_at: z.string(),\n  response: z.string(),\n  done: z.boolean(),\n  context: z.array(z.number()),\n\n  eval_count: z.number().optional(),\n  eval_duration: z.number().optional(),\n\n  load_duration: z.number().optional(),\n  total_duration: z.number().optional(),\n\n  prompt_eval_count: z.number().optional(),\n  prompt_eval_duration: z.number().optional(),\n});\n","import { z } from 'zod/v4';\nimport { createJsonErrorResponseHandler } from '@ai-sdk/provider-utils';\n\nexport const ollamaErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n\n    // The additional information below is handled loosely to support\n    // Ollama-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish(),\n  }),\n});\n\nexport type OllamaErrorData = z.infer<typeof ollamaErrorDataSchema>;\n\nexport const ollamaFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: ollamaErrorDataSchema,\n  errorToMessage: data => data.error.message,\n});\n","import {\n  InvalidPromptError,\n  LanguageModelV2Prompt,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport function convertToOllamaCompletionPrompt({\n  prompt,\n  user = 'user',\n  assistant = 'assistant',\n}: {\n  prompt: LanguageModelV2Prompt;\n  user?: string;\n  assistant?: string;\n}): {\n  prompt: string;\n  stopSequences?: string[];\n} {\n  // transform to a chat message format:\n  let text = '';\n\n  // if first message is a system message, add it to the text:\n  if (prompt[0].role === 'system') {\n    text += `${prompt[0].content}\\n\\n`;\n    prompt = prompt.slice(1);\n  }\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        throw new InvalidPromptError({\n          message: 'Unexpected system message in prompt: ${content}',\n          prompt,\n        });\n      }\n\n      case 'user': {\n        const userMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n            }\n          })\n          .filter(Boolean)\n          .join('');\n\n        text += `${user}:\\n${userMessage}\\n\\n`;\n        break;\n      }\n\n      case 'assistant': {\n        const assistantMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n              case 'tool-call': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'tool-call messages',\n                });\n              }\n            }\n          })\n          .join('');\n\n        text += `${assistant}:\\n${assistantMessage}\\n\\n`;\n        break;\n      }\n\n      case 'tool': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'tool messages',\n        });\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  // Assistant message prefix:\n  text += `${assistant}:\\n`;\n\n  return {\n    prompt: text,\n    stopSequences: [`\\n${user}:`],\n  };\n}\n","import { LanguageModelV2FinishReason } from '@ai-sdk/provider';\n\nexport function mapOllamaFinishReason(\n  finishReason: string | null | undefined,\n): LanguageModelV2FinishReason {\n  switch (finishReason) {\n    case 'stop':\n      return 'stop';\n    case 'length':\n      return 'length';\n    case 'content_filter':\n      return 'content-filter';\n    case 'function_call':\n    case 'tool_calls':\n      return 'tool-calls';\n    default:\n      return 'unknown';\n  }\n}\n","export function getResponseMetadata({\n  model,\n  created_at,\n}: {\n  created_at?: string | undefined | null;\n  model?: string | undefined | null;\n}) {\n  return {\n    id: undefined,\n    modelId: model ?? undefined,\n    timestamp: created_at != null ? new Date(created_at) : undefined,\n  };\n}\n","import {\n  EmbeddingModelV2,\n  TooManyEmbeddingValuesForCallError,\n} from \"@ai-sdk/provider\";\nimport {\n  combineHeaders,\n  createJsonResponseHandler,\n  parseProviderOptions,\n  postJsonToApi,\n} from \"@ai-sdk/provider-utils\";\nimport { z } from \"zod/v4\";\nimport { OllamaConfig } from \"../common/ollama-config\";\nimport {\n  OllamaEmbeddingModelId,\n  OllamaEmbeddingSettings,\n} from \"./ollama-embedding-settings\";\nimport { ollamaFailedResponseHandler } from \"../completion/ollama-error\";\n\nconst ollamaEmbeddingProviderOptions = z.object({\n  dimensions: z.number().optional(),\n  truncate: z.boolean().optional(),\n  keepAlive: z.string().optional(),\n});\n\nexport type OllamaEmbeddingProviderOptions = z.infer<typeof ollamaEmbeddingProviderOptions>;\n\nexport class OllamaEmbeddingModel implements EmbeddingModelV2<string> {\n  readonly specificationVersion = \"v2\";\n  readonly modelId: OllamaEmbeddingModelId;\n\n  private readonly config: OllamaConfig;\n  private readonly settings: OllamaEmbeddingSettings;\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  get maxEmbeddingsPerCall(): number {\n    return this.settings.maxEmbeddingsPerCall ?? 2048;\n  }\n\n  get supportsParallelCalls(): boolean {\n    return this.settings.supportsParallelCalls ?? true;\n  }\n\n  constructor(\n    modelId: OllamaEmbeddingModelId,\n    settings: OllamaEmbeddingSettings,\n    config: OllamaConfig\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n\n  private async getArgs({\n    values,\n    providerOptions,\n  }: Parameters<EmbeddingModelV2<string>[\"doEmbed\"]>[0]) {\n    // Parse provider options\n    const ollamaOptions =\n      (await parseProviderOptions({\n        provider: \"ollama\",\n        providerOptions,\n        schema: ollamaEmbeddingProviderOptions,\n      })) ?? {};\n\n    return {\n      args: {\n        // model id:\n        model: this.modelId,\n        input: values,\n\n        // advanced parameters:\n        dimensions: ollamaOptions.dimensions ?? this.settings.dimensions,\n        truncate: ollamaOptions.truncate,\n        keep_alive: ollamaOptions.keepAlive,\n      }\n    };\n  }\n\n  async doEmbed({\n    values,\n    headers,\n    abortSignal,\n    providerOptions,\n  }: Parameters<EmbeddingModelV2<string>[\"doEmbed\"]>[0]): Promise<\n    Awaited<ReturnType<EmbeddingModelV2<string>[\"doEmbed\"]>>\n  > {\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values,\n      });\n    }\n\n    const { args: body } = await this.getArgs({values, providerOptions})\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/embed\",\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: { ...body },\n      failedResponseHandler: ollamaFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        ollamaTextEmbeddingResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      embeddings: response.embeddings.map((item) => item),\n      usage: { tokens: response.prompt_eval_count },\n      response: { headers: responseHeaders, body: rawValue },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst ollamaTextEmbeddingResponseSchema = z.object({\n  model: z.string(),\n  embeddings: z.array(z.array(z.number())),\n  total_duration: z.number(),\n  load_duration: z.number(),\n  prompt_eval_count: z.number(),\n});\n","import {\n  LanguageModelV2,\n  LanguageModelV2CallWarning,\n} from \"@ai-sdk/provider\";\nimport {\n  combineHeaders,\n  createJsonResponseHandler,\n  createJsonStreamResponseHandler,\n  postJsonToApi,\n} from \"@ai-sdk/provider-utils\";\nimport { OllamaConfig } from \"../common/ollama-config\";\nimport { ollamaFailedResponseHandler } from \"../completion/ollama-error\";\nimport { OllamaChatModelId } from \"../ollama-chat-settings\";\nimport { \n  OllamaRequestBuilder,\n  OllamaResponsesProviderOptions \n} from \"./ollama-responses-request-builder\";\nimport { \n  OllamaResponseProcessor, \n  baseOllamaResponseSchema \n} from \"./ollama-responses-processor\";\nimport { OllamaStreamProcessor } from \"./ollama-responses-stream-processor\";\n\nexport class OllamaResponsesLanguageModel implements LanguageModelV2 {\n  readonly specificationVersion = \"v2\";\n  readonly modelId: OllamaChatModelId;\n\n  private readonly config: OllamaConfig;\n  private readonly requestBuilder: OllamaRequestBuilder;\n  private readonly responseProcessor: OllamaResponseProcessor;\n\n  constructor(modelId: OllamaChatModelId, config: OllamaConfig) {\n    this.modelId = modelId;\n    this.config = config;\n    this.requestBuilder = new OllamaRequestBuilder();\n    this.responseProcessor = new OllamaResponseProcessor(config);\n  }\n\n  readonly supportedUrls: Record<string, RegExp[]> = {\n    'image/*': [\n      /^https?:\\/\\/.*$/\n    ]\n  };\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV2[\"doGenerate\"]>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2[\"doGenerate\"]>>> {\n    const { args: body, warnings } = await this.prepareRequest(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat\",\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: { ...body, stream: false },\n      failedResponseHandler: ollamaFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(baseOllamaResponseSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const processedResponse = this.responseProcessor.processGenerateResponse(response);\n\n    return {\n      ...processedResponse,\n      request: { body: JSON.stringify(body) },\n      response: {\n        modelId: this.modelId,\n        timestamp: new Date(),\n        headers: responseHeaders,\n        body: rawResponse,\n      },\n      warnings,\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV2[\"doStream\"]>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2[\"doStream\"]>>> {\n    const { args: body, warnings } = await this.prepareRequest(options);\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat\",\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: { ...body, stream: true },\n      failedResponseHandler: ollamaFailedResponseHandler,\n      successfulResponseHandler: createJsonStreamResponseHandler(baseOllamaResponseSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const streamProcessor = new OllamaStreamProcessor(this.config);\n\n    return {\n      stream: response.pipeThrough(\n        streamProcessor.createTransformStream(warnings, options)\n      ),\n      request: { body },\n      response: { headers: responseHeaders },\n    };\n  }\n\n  private async prepareRequest(options: Parameters<LanguageModelV2[\"doGenerate\"]>[0]) {\n    return await this.requestBuilder.buildRequest({\n      modelId: this.modelId,\n      ...options,\n    });\n  }\n}\n\n// Re-export types for convenience\nexport type { OllamaResponsesProviderOptions };\n","import {\n  LanguageModelV2CallWarning,\n  LanguageModelV2,\n} from \"@ai-sdk/provider\";\nimport { parseProviderOptions } from \"@ai-sdk/provider-utils\";\nimport { z } from \"zod/v4\";\nimport { convertToOllamaResponsesMessages } from \"./convert-to-ollama-responses-messages\";\nimport { convertToOllamaChatMessages } from \"../adaptors/convert-to-ollama-chat-messages\";\nimport { prepareResponsesTools } from \"./ollama-responses-prepare-tools\";\nimport { OllamaChatModelId, ollamaProviderOptions } from \"../ollama-chat-settings\";\n\n\nexport type OllamaResponsesProviderOptions = z.infer<\n  typeof ollamaProviderOptions\n>;\n\ninterface RequestBuilderOptions {\n  modelId: OllamaChatModelId;\n  maxOutputTokens?: number;\n  temperature?: number;\n  stopSequences?: string[];\n  topP?: number;\n  topK?: number;\n  presencePenalty?: number;\n  frequencyPenalty?: number;\n  seed?: number;\n  prompt: any;\n  providerOptions?: any;\n  tools?: any;\n  toolChoice?: any;\n  responseFormat?: any;\n}\n\ninterface RequestBuilderResult {\n  args: {\n    model: OllamaChatModelId;\n    messages: any;\n    temperature?: number;\n    top_p?: number;\n    max_output_tokens?: number;\n    format?: any;\n    user?: string;\n    think?: boolean;\n    tools?: any;\n    tool_choice?: any;\n  };\n  warnings: LanguageModelV2CallWarning[];\n}\n\nexport class OllamaRequestBuilder {\n  async buildRequest({\n    modelId,\n    maxOutputTokens,\n    temperature,\n    stopSequences,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    seed,\n    prompt,\n    providerOptions,\n    tools,\n    toolChoice,\n    responseFormat,\n  }: RequestBuilderOptions): Promise<RequestBuilderResult> {\n    const warnings = this.collectUnsupportedSettingsWarnings({\n      topK,\n      seed,\n      presencePenalty,\n      frequencyPenalty,\n      stopSequences,\n    });\n\n    const { messages, warnings: messageWarnings } =\n      convertToOllamaResponsesMessages({\n        prompt,\n        systemMessageMode: \"system\",\n      });\n\n    warnings.push(...messageWarnings);\n\n    const ollamaOptions = await this.parseProviderOptions(providerOptions);\n\n    const baseArgs = this.buildBaseArgs({\n      modelId,\n      prompt,\n      temperature,\n      topP,\n      maxOutputTokens,\n      responseFormat,\n      ollamaOptions,\n    });\n\n    const { tools: ollamaTools, toolChoice: ollamaToolChoice, toolWarnings } =\n      prepareResponsesTools({\n        tools,\n        toolChoice,\n      });\n\n    return {\n      args: {\n        ...baseArgs,\n        tools: ollamaTools,\n        tool_choice: ollamaToolChoice,\n      },\n      warnings: [...warnings, ...toolWarnings],\n    };\n  }\n\n  private collectUnsupportedSettingsWarnings({\n    topK,\n    seed,\n    presencePenalty,\n    frequencyPenalty,\n    stopSequences,\n  }: {\n    topK?: number;\n    seed?: number;\n    presencePenalty?: number;\n    frequencyPenalty?: number;\n    stopSequences?: string[];\n  }): LanguageModelV2CallWarning[] {\n    const warnings: LanguageModelV2CallWarning[] = [];\n\n    const unsupportedSettings = [\n      { value: topK, name: \"topK\" },\n      { value: seed, name: \"seed\" },\n      { value: presencePenalty, name: \"presencePenalty\" },\n      { value: frequencyPenalty, name: \"frequencyPenalty\" },\n      { value: stopSequences, name: \"stopSequences\" },\n    ] as const;\n\n    for (const { value, name } of unsupportedSettings) {\n      if (value != null) {\n        warnings.push({ type: \"unsupported-setting\", setting: name });\n      }\n    }\n\n    return warnings;\n  }\n\n  private async parseProviderOptions(providerOptions: any): Promise<OllamaResponsesProviderOptions | null> {\n    const result = await parseProviderOptions({\n      provider: \"ollama\",\n      providerOptions,\n      schema: ollamaProviderOptions,\n    });\n    return result ?? null;\n  }\n\n  private buildBaseArgs({\n    modelId,\n    prompt,\n    temperature,\n    topP,\n    maxOutputTokens,\n    responseFormat,\n    ollamaOptions,\n  }: {\n    modelId: OllamaChatModelId;\n    prompt: any;\n    temperature?: number;\n    topP?: number;\n    maxOutputTokens?: number;\n    responseFormat?: any;\n    ollamaOptions: OllamaResponsesProviderOptions | null;\n  }) {\n    return {\n      model: modelId,\n      messages: convertToOllamaChatMessages({\n        prompt,\n        systemMessageMode: \"system\",\n      }),\n      temperature,\n      top_p: topP,\n      max_output_tokens: maxOutputTokens,\n\n      ...(responseFormat?.type === \"json\" && {\n        format: responseFormat.schema != null ? responseFormat.schema : \"json\",\n      }),\n\n      think: ollamaOptions?.think ?? false,\n      options: ollamaOptions?.options?? undefined\n    };\n  }\n} ","import {\n    LanguageModelV2CallWarning,\n    LanguageModelV2Prompt,\n    UnsupportedFunctionalityError,\n  } from '@ai-sdk/provider';\n  import { OllamaResponsesPrompt } from './ollama-responses-api-types';\n  \n  export function convertToOllamaResponsesMessages({\n    prompt,\n    systemMessageMode,\n  }: {\n    prompt: LanguageModelV2Prompt;\n    systemMessageMode: 'system' | 'developer' | 'remove';\n  }): {\n    messages: OllamaResponsesPrompt;\n    warnings: Array<LanguageModelV2CallWarning>;\n  } {\n    const messages: OllamaResponsesPrompt = [];\n    const warnings: Array<LanguageModelV2CallWarning> = [];\n  \n    for (const { role, content } of prompt) {\n      switch (role) {\n        case 'system': {\n          switch (systemMessageMode) {\n            case 'system': {\n              messages.push({ role: 'system', content });\n              break;\n            }\n            case 'developer': {\n              messages.push({ role: 'developer', content });\n              break;\n            }\n            case 'remove': {\n              warnings.push({\n                type: 'other',\n                message: 'system messages are removed for this model',\n              });\n              break;\n            }\n            default: {\n              const _exhaustiveCheck: never = systemMessageMode;\n              throw new Error(\n                `Unsupported system message mode: ${_exhaustiveCheck}`,\n              );\n            }\n          }\n          break;\n        }\n  \n        case 'user': {\n          messages.push({\n            role: 'user',\n            content: content.map((part, index) => {\n              switch (part.type) {\n                case 'text': {\n                  return { type: 'input_text', text: part.text };\n                }\n                case 'file': {\n                  if (part.mediaType.startsWith('image/')) {\n                    const mediaType =\n                      part.mediaType === 'image/*'\n                        ? 'image/jpeg'\n                        : part.mediaType;\n  \n                    return {\n                      type: 'input_image',\n                      image_url:\n                        part.data instanceof URL\n                          ? part.data.toString()\n                          : `data:${mediaType};base64,${part.data}`,\n  \n                      // Ollama specific extension: image detail\n                      detail: part.providerOptions?.ollama?.imageDetail,\n                    };\n                  } else if (part.mediaType === 'application/pdf') {\n                    if (part.data instanceof URL) {\n                      // The AI SDK automatically downloads files for user file parts with URLs\n                      throw new UnsupportedFunctionalityError({\n                        functionality: 'PDF file parts with URLs',\n                      });\n                    }\n  \n                    return {\n                      type: 'input_file',\n                      filename: part.filename ?? `part-${index}.pdf`,\n                      file_data: `data:application/pdf;base64,${part.data}`,\n                    };\n                  } else {\n                    throw new UnsupportedFunctionalityError({\n                      functionality: `file part media type ${part.mediaType}`,\n                    });\n                  }\n                }\n              }\n            }),\n          });\n  \n          break;\n        }\n  \n        case 'assistant': {\n          for (const part of content) {\n            switch (part.type) {\n              case 'text': {\n                messages.push({\n                  role: 'assistant',\n                  content: [{ type: 'output_text', text: part.text }],\n                });\n                break;\n              }\n              case 'tool-call': {\n                if (part.providerExecuted) {\n                  break;\n                }\n  \n                messages.push({\n                  type: 'function_call',\n                  call_id: part.toolCallId,\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.input),\n                });\n                break;\n              }\n  \n              case 'tool-result': {\n                warnings.push({\n                  type: 'other',\n                  message: `tool result parts in assistant messages are not supported for Ollama responses`,\n                });\n                break;\n              }\n            }\n          }\n  \n          break;\n        }\n  \n        case 'tool': {\n          for (const part of content) {\n            const output = part.output;\n  \n            let contentValue: string;\n            switch (output.type) {\n              case 'text':\n              case 'error-text':\n                contentValue = output.value;\n                break;\n              case 'content':\n              case 'json':\n              case 'error-json':\n                contentValue = JSON.stringify(output.value);\n                break;\n            }\n  \n            messages.push({\n              type: 'function_call_output',\n              call_id: part.toolCallId,\n              output: contentValue,\n            });\n          }\n  \n          break;\n        }\n  \n        default: {\n          const _exhaustiveCheck: never = role;\n          throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n        }\n      }\n    }\n  \n    return { messages, warnings };\n  }\n  ","import {\n  LanguageModelV2FilePart,\n  LanguageModelV2Prompt,\n} from '@ai-sdk/provider';\nimport { OllamaChatPrompt } from './ollama-chat-prompt';\n\nexport function convertToOllamaChatMessages({\n  prompt,\n  systemMessageMode = 'system',\n}: {\n  prompt: LanguageModelV2Prompt;\n  systemMessageMode?: 'system' | 'developer' | 'remove';\n}): OllamaChatPrompt {\n  const messages: OllamaChatPrompt = [];\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        switch (systemMessageMode) {\n          case 'system': {\n            messages.push({ role: 'system', content });\n            break;\n          }\n          case 'developer': {\n            messages.push({ role: 'developer', content });\n            break;\n          }\n          case 'remove': {\n            break;\n          }\n          default: {\n            const _exhaustiveCheck: never = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`,\n            );\n          }\n        }\n        break;\n      }\n\n      case 'user': {\n        if (content.length === 1 && content[0].type === 'text') {\n          messages.push({ role: 'user', content: content[0].text });\n          break;\n        }\n\n        const userText = content.filter((part) => part.type === 'text').map((part) => part.text).join('');\n        const images = content\n          .filter((part) => part.type === 'file' && part.mediaType.startsWith('image/'))\n          .map((part) => (part as LanguageModelV2FilePart).data);\n\n        messages.push({\n          role: 'user',\n          content: userText.length > 0 ? userText : [],\n          images: images.length > 0 ? images : undefined\n        });\n\n        break;\n      }\n\n      case 'assistant': {\n        let text = '';\n        let thinking = '';\n        const toolCalls: Array<{\n          id: string;\n          type: 'function';\n          function: { name: string; arguments: object };\n        }> = [];\n\n        for (const part of content) {\n          switch (part.type) {\n            case 'text': {\n              text += part.text;\n              break;\n            }\n            case 'tool-call': {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: 'function',\n                function: {\n                  name: part.toolName,\n                  arguments: part.input as object,\n                },\n              });\n              break;\n            }\n            case 'reasoning': {\n              thinking += part.text;\n              break;\n            }\n            default: {\n              throw new Error(`Unsupported part: ${part}`);\n            }\n          }\n        }\n\n        messages.push({\n          role: 'assistant',\n          content: text,\n          ...(thinking && { thinking }),\n          tool_calls: toolCalls.length > 0 ? toolCalls : undefined,\n        });\n\n        break;\n      }\n\n      case 'tool': {\n        for (const toolResponse of content) {\n          const output = toolResponse.output;\n\n          let contentValue: string;\n          switch (output.type) {\n            case 'text':\n            case 'error-text':\n              contentValue = output.value;\n              break;\n            case 'content':\n            case 'json':\n            case 'error-json':\n              contentValue = JSON.stringify(output.value);\n              break;\n          }\n\n          messages.push({\n            role: 'tool',\n            tool_call_id: toolResponse.toolCallId,\n            content: contentValue,\n          });\n        }\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  return messages;\n}\n","import {\n  LanguageModelV2CallOptions,\n  LanguageModelV2CallWarning,\n  UnsupportedFunctionalityError,\n} from \"@ai-sdk/provider\";\nimport { OllamaResponsesTool } from \"./ollama-responses-api-types\";\n\nexport function prepareResponsesTools({\n  tools,\n  toolChoice,\n}: {\n  tools: LanguageModelV2CallOptions[\"tools\"];\n  toolChoice?: LanguageModelV2CallOptions[\"toolChoice\"];\n}): {\n  tools?: Array<OllamaResponsesTool>;\n  toolChoice?:\n    | \"auto\"\n    | \"none\"\n    | \"required\"\n    | { type: \"web_search_preview\" }\n    | { type: \"function\"; name: string };\n  toolWarnings: LanguageModelV2CallWarning[];\n} {\n  // when the tools array is empty, change it to undefined to prevent errors:\n  tools = tools?.length ? tools : undefined;\n\n  const toolWarnings: LanguageModelV2CallWarning[] = [];\n\n  if (tools == null) {\n    return { tools: undefined, toolChoice: undefined, toolWarnings };\n  }\n\n  const ollamaTools: Array<OllamaResponsesTool> = [];\n\n  for (const tool of tools) {\n    switch (tool.type) {\n      case \"function\": {\n        // Ensure parameters is always a non-null object (even if empty)\n        let parameters = tool.inputSchema;\n        if(!parameters){\n          parameters = {\n            type: \"object\",\n            properties: {},\n            required: [],\n          };\n        }\n        else if (\n          parameters &&\n          typeof parameters === \"object\" &&\n          parameters.type === \"object\" &&\n          parameters.properties &&\n          Object.keys(parameters.properties).length === 0\n        ) {\n          // Defensive: ensure required/optional fields are handled for empty schemas\n          parameters = {\n            ...parameters,\n            properties: {},\n            required: [],\n          };\n        } \n        \n        ollamaTools.push({\n          type: \"function\",\n          function: {\n            name: tool.name,\n            description: tool.description,\n            parameters,\n          },\n        });\n        break;\n      }\n      default:\n        toolWarnings.push({ type: \"unsupported-tool\", tool });\n        break;\n    }\n  }\n\n  if (toolChoice == null) {\n    return { tools: ollamaTools, toolChoice: undefined, toolWarnings };\n  }\n\n  const type = toolChoice.type;\n\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return { tools: ollamaTools, toolChoice: type, toolWarnings };\n    case \"tool\":\n      return {\n        tools: ollamaTools,\n        toolChoice:\n          toolChoice.toolName == \"web_search_preview\"\n            ? { type: \"web_search_preview\" }\n            : { type: \"function\", name: toolChoice.toolName },\n        toolWarnings,\n      };\n    default: {\n      const _exhaustiveCheck: never = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `tool choice type: ${_exhaustiveCheck}`,\n      });\n    }\n  }\n}\n","import { z } from \"zod/v4\";\n\n// https://platform.ollama.com/docs/models\nexport type OllamaChatModelId =\n  | \"athene-v2\"\n  | \"athene-v2:72b\"\n  | \"aya-expanse\"\n  | \"aya-expanse:8b\"\n  | \"aya-expanse:32b\"\n  | \"codegemma\"\n  | \"codegemma:2b\"\n  | \"codegemma:7b\"\n  | \"codellama\"\n  | \"codellama:7b\"\n  | \"codellama:13b\"\n  | \"codellama:34b\"\n  | \"codellama:70b\"\n  | \"codellama:code\"\n  | \"codellama:python\"\n  | \"command-r\"\n  | \"command-r:35b\"\n  | \"command-r-plus\"\n  | \"command-r-plus:104b\"\n  | \"command-r7b\"\n  | \"command-r7b:7b\"\n  | \"deepseek-r1\"\n  | \"deepseek-r1:1.5b\"\n  | \"deepseek-r1:7b\"\n  | \"deepseek-r1:8b\"\n  | \"deepseek-r1:14b\"\n  | \"deepseek-r1:32b\"\n  | \"deepseek-r1:70b\"\n  | \"deepseek-r1:671b\"\n  | \"deepseek-coder-v2\"\n  | \"deepseek-coder-v2:16b\"\n  | \"deepseek-coder-v2:236b\"\n  | \"deepseek-v3\"\n  | \"deepseek-v3:671b\"\n  | \"devstral\"\n  | \"devstral:24b\"\n  | \"dolphin3\"\n  | \"dolphin3:8b\"\n  | \"exaone3.5\"\n  | \"exaone3.5:2.4b\"\n  | \"exaone3.5:7.8b\"\n  | \"exaone3.5:32b\"\n  | \"falcon2\"\n  | \"falcon2:11b\"\n  | \"falcon3\"\n  | \"falcon3:1b\"\n  | \"falcon3:3b\"\n  | \"falcon3:7b\"\n  | \"falcon3:10b\"\n  | \"firefunction-v2\"\n  | \"firefunction-v2:70b\"\n  | \"gemma\"\n  | \"gemma:2b\"\n  | \"gemma:7b\"\n  | \"gemma2\"\n  | \"gemma2:2b\"\n  | \"gemma2:9b\"\n  | \"gemma2:27b\"\n  | \"gemma3\"\n  | \"gemma3:1b\"\n  | \"gemma3:4b\"\n  | \"gemma3:12b\"\n  | \"gemma3:27b\"\n  | \"granite3-dense\"\n  | \"granite3-dense:2b\"\n  | \"granite3-dense:8b\"\n  | \"granite3-guardian\"\n  | \"granite3-guardian:2b\"\n  | \"granite3-guardian:8b\"\n  | \"granite3-moe\"\n  | \"granite3-moe:1b\"\n  | \"granite3-moe:3b\"\n  | \"granite3.1-dense\"\n  | \"granite3.1-dense:2b\"\n  | \"granite3.1-dense:8b\"\n  | \"granite3.1-moe\"\n  | \"granite3.1-moe:1b\"\n  | \"granite3.1-moe:3b\"\n  | \"llama2\"\n  | \"llama2:7b\"\n  | \"llama2:13b\"\n  | \"llama2:70b\"\n  | \"llama3\"\n  | \"llama3:8b\"\n  | \"llama3:70b\"\n  | \"llama3-chatqa\"\n  | \"llama3-chatqa:8b\"\n  | \"llama3-chatqa:70b\"\n  | \"llama3-gradient\"\n  | \"llama3-gradient:8b\"\n  | \"llama3-gradient:70b\"\n  | \"llama3.1\"\n  | \"llama3.1:8b\"\n  | \"llama3.1:70b\"\n  | \"llama3.1:405b\"\n  | \"llama3.2\"\n  | \"llama3.2:1b\"\n  | \"llama3.2:3b\"\n  | \"llama3.2-vision\"\n  | \"llama3.2-vision:11b\"\n  | \"llama3.2-vision:90b\"\n  | \"llama3.3\"\n  | \"llama3.3:70b\"\n  | \"llama4\"\n  | \"llama4:16x17b\"\n  | \"llama4:128x17b\"\n  | \"llama-guard3\"\n  | \"llama-guard3:1b\"\n  | \"llama-guard3:8b\"\n  | \"llava\"\n  | \"llava:7b\"\n  | \"llava:13b\"\n  | \"llava:34b\"\n  | \"llava-llama3\"\n  | \"llava-llama3:8b\"\n  | \"llava-phi3\"\n  | \"llava-phi3:3.8b\"\n  | \"marco-o1\"\n  | \"marco-o1:7b\"\n  | \"mistral\"\n  | \"mistral:7b\"\n  | \"mistral-large\"\n  | \"mistral-large:123b\"\n  | \"mistral-nemo\"\n  | \"mistral-nemo:12b\"\n  | \"mistral-small\"\n  | \"mistral-small:22b\"\n  | \"mixtral\"\n  | \"mixtral:8x7b\"\n  | \"mixtral:8x22b\"\n  | \"moondream\"\n  | \"moondream:1.8b\"\n  | \"openhermes\"\n  | \"openhermes:v2.5\"\n  | \"nemotron\"\n  | \"nemotron:70b\"\n  | \"nemotron-mini\"\n  | \"nemotron-mini:4b\"\n  | \"olmo\"\n  | \"olmo:7b\"\n  | \"olmo:13b\"\n  | \"opencoder\"\n  | \"opencoder:1.5b\"\n  | \"opencoder:8b\"\n  | \"phi3\"\n  | \"phi3:3.8b\"\n  | \"phi3:14b\"\n  | \"phi3.5\"\n  | \"phi3.5:3.8b\"\n  | \"phi4\"\n  | \"phi4:14b\"\n  | \"qwen\"\n  | \"qwen:7b\"\n  | \"qwen:14b\"\n  | \"qwen:32b\"\n  | \"qwen:72b\"\n  | \"qwen:110b\"\n  | \"qwen2\"\n  | \"qwen2:0.5b\"\n  | \"qwen2:1.5b\"\n  | \"qwen2:7b\"\n  | \"qwen2:72b\"\n  | \"qwen2.5\"\n  | \"qwen2.5:0.5b\"\n  | \"qwen2.5:1.5b\"\n  | \"qwen2.5:3b\"\n  | \"qwen2.5:7b\"\n  | \"qwen2.5:14b\"\n  | \"qwen2.5:32b\"\n  | \"qwen2.5:72b\"\n  | \"qwen2.5-coder\"\n  | \"qwen2.5-coder:0.5b\"\n  | \"qwen2.5-coder:1.5b\"\n  | \"qwen2.5-coder:3b\"\n  | \"qwen2.5-coder:7b\"\n  | \"qwen2.5-coder:14b\"\n  | \"qwen2.5-coder:32b\"\n  | \"qwen3\"\n  | \"qwen3:0.6b\"\n  | \"qwen3:1.7b\"\n  | \"qwen3:4b\"\n  | \"qwen3:8b\"\n  | \"qwen3:14b\"\n  | \"qwen3:30b\"\n  | \"qwen3:32b\"\n  | \"qwen3:235b\"\n  | \"qwq\"\n  | \"qwq:32b\"\n  | \"sailor2\"\n  | \"sailor2:1b\"\n  | \"sailor2:8b\"\n  | \"sailor2:20b\"\n  | \"shieldgemma\"\n  | \"shieldgemma:2b\"\n  | \"shieldgemma:9b\"\n  | \"shieldgemma:27b\"\n  | \"smallthinker\"\n  | \"smallthinker:3b\"\n  | \"smollm\"\n  | \"smollm:135m\"\n  | \"smollm:360m\"\n  | \"smollm:1.7b\"\n  | \"tinyllama\"\n  | \"tinyllama:1.1b\"\n  | \"tulu3\"\n  | \"tulu3:8b\"\n  | \"tulu3:70b\"\n  | (string & {});\n\nexport const ollamaProviderOptions = z.object({\n  /**\n   * Enable or disable the model's thinking process. When enabled, the output will separate\n   * the model's thinking from the model's output. When disabled, the model will not think\n   * and directly output the content.\n   *\n   * Only supported by certain models like DeepSeek R1 and Qwen 3.\n   */\n  think: z.boolean().optional(),\n  options: z.object({\n    num_ctx: z.number().optional(),\n    repeat_last_n: z.number().optional(),\n    repeat_penalty: z.number().optional(),\n    temperature: z.number().optional(),\n    seed: z.number().optional(),\n    stop: z.array(z.string()).optional(),\n    num_predict: z.number().optional(),\n    top_k: z.number().optional(),\n    top_p: z.number().optional(),\n    min_p: z.number().optional(),\n  }).optional()\n\n\n});\n\nexport type OllamaProviderOptions = z.infer<typeof ollamaProviderOptions>;\n","import {\n  LanguageModelV2Content,\n  LanguageModelV2FinishReason,\n  LanguageModelV2Usage,\n  SharedV2ProviderMetadata,\n} from \"@ai-sdk/provider\";\nimport { generateId } from \"@ai-sdk/provider-utils\";\nimport { z } from \"zod/v4\";\nimport { mapOllamaFinishReason } from \"../adaptors/map-ollama-finish-reason\";\nimport { OllamaConfig } from \"../common/ollama-config\";\n\nexport const baseOllamaResponseSchema = z.object({\n  model: z.string(),\n  created_at: z.string(),\n  done: z.boolean(),\n  message: z.object({\n    content: z.string(),\n    role: z.string(),\n    thinking: z.string().optional(),\n    tool_calls: z\n      .array(\n        z.object({\n          function: z.object({\n            name: z.string(),\n            arguments: z.record(z.string(), z.any()),\n          }),\n          id: z.string().optional(),\n        }),\n      )\n      .optional()\n      .nullable(),\n  }),\n\n  done_reason: z.string().optional(),\n  eval_count: z.number().optional(),\n  eval_duration: z.number().optional(),\n  load_duration: z.number().optional(),\n  prompt_eval_count: z.number().optional(),\n  prompt_eval_duration: z.number().optional(),\n  total_duration: z.number().optional(),\n});\n\nexport type OllamaResponse = z.infer<typeof baseOllamaResponseSchema>;\n\nexport class OllamaResponseProcessor {\n  constructor(private config: OllamaConfig) {}\n\n  processGenerateResponse(response: OllamaResponse): {\n    content: LanguageModelV2Content[];\n    finishReason: LanguageModelV2FinishReason;\n    usage: LanguageModelV2Usage;\n    providerMetadata: SharedV2ProviderMetadata;\n  } {\n    const content = this.extractContent(response);\n    const finishReason = mapOllamaFinishReason(response.done_reason);\n    const usage = this.extractUsage(response);\n    const providerMetadata: SharedV2ProviderMetadata = { ollama: {} };\n\n    return {\n      content,\n      finishReason,\n      usage,\n      providerMetadata,\n    };\n  }\n\n  private extractContent(response: OllamaResponse): LanguageModelV2Content[] {\n    const content: LanguageModelV2Content[] = [];\n\n    // Add text content\n    const text = response.message.content;\n    if (text != null && text.length > 0) {\n      content.push({\n        type: \"text\",\n        text,\n      });\n    }\n\n    // Add tool calls\n    for (const toolCall of response.message.tool_calls ?? []) {\n      content.push({\n        type: \"tool-call\" as const,\n        toolCallId: toolCall.id ?? (this.config.generateId?.() ?? generateId()),\n        toolName: toolCall.function.name,\n        input: JSON.stringify(toolCall.function.arguments),\n      });\n    }\n\n    return content;\n  }\n\n  private extractUsage(response: OllamaResponse): LanguageModelV2Usage {\n    return {\n      inputTokens: response.prompt_eval_count ?? undefined,\n      outputTokens: response.eval_count ?? undefined,\n      totalTokens: (response.prompt_eval_count ?? 0) + (response.eval_count ?? 0),\n      reasoningTokens: undefined, // Ollama doesn't provide separate reasoning tokens\n      cachedInputTokens: undefined,\n    };\n  }\n}\n\n/**\n * Extracts one or more valid Ollama response objects from a stream chunk.\n * Handles both successful parsed chunks and error chunks that may contain\n * multiple JSON objects separated by newlines (NDJSON-like behavior).\n */\nexport function extractOllamaResponseObjectsFromChunk(\n  chunk: any,\n): OllamaResponse[] {\n  if (chunk.success) {\n    return [chunk.value];\n  }\n\n  const results: OllamaResponse[] = [];\n  const raw = (chunk.error as any)?.text;\n  if (typeof raw !== \"string\" || raw.length === 0) {\n    return results;\n  }\n\n  const lines = raw.split(/\\r?\\n/);\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (trimmed === \"\") continue;\n    try {\n      const parsed = JSON.parse(trimmed);\n      const validated = baseOllamaResponseSchema.safeParse(parsed);\n      if (validated.success) {\n        results.push(validated.data);\n      }\n    } catch {\n      // Ignore malformed line; continue with remaining lines\n    }\n  }\n\n  return results;\n} ","import {\n  InvalidResponseDataError,\n  LanguageModelV2FinishReason,\n  LanguageModelV2StreamPart,\n  LanguageModelV2Usage,\n} from \"@ai-sdk/provider\";\nimport { generateId, ParseResult } from \"@ai-sdk/provider-utils\";\nimport { z } from \"zod/v4\";\nimport { OllamaConfig } from \"../common/ollama-config\";\nimport { getResponseMetadata } from \"../common/get-response-metadata\";\nimport { mapOllamaFinishReason } from \"../adaptors/map-ollama-finish-reason\";\nimport {\n  baseOllamaResponseSchema,\n  OllamaResponse,\n  extractOllamaResponseObjectsFromChunk,\n} from \"./ollama-responses-processor\";\n\ninterface StreamState {\n  finishReason: LanguageModelV2FinishReason;\n  usage: LanguageModelV2Usage;\n  responseId: string | null;\n  ongoingToolCalls: Record<number, { toolName: string; toolCallId: string } | undefined>;\n  hasToolCalls: boolean;\n  isFirstChunk: boolean;\n  hasTextStarted: boolean;\n  hasReasoningStarted: boolean;\n  textEnded: boolean;\n  reasoningEnded: boolean;\n  textId: string;\n}\n\nexport class OllamaStreamProcessor {\n  private state: StreamState;\n\n  constructor(private config: OllamaConfig) {\n    this.state = this.initializeState();\n  }\n\n  createTransformStream(warnings: any[], options: any): TransformStream<\n    ParseResult<z.infer<typeof baseOllamaResponseSchema>>,\n    LanguageModelV2StreamPart\n  > {\n    return new TransformStream({\n      start: (controller) => {\n        controller.enqueue({ type: \"stream-start\", warnings });\n      },\n\n      transform: (chunk, controller) => {\n        this.processChunk(chunk, controller, options);\n      },\n\n      flush: (controller) => {\n        this.finalizeStream(controller);\n      },\n    });\n  }\n\n  private initializeState(): StreamState {\n    return {\n      finishReason: \"unknown\",\n      usage: {\n        inputTokens: undefined,\n        outputTokens: undefined,\n        totalTokens: undefined,\n      },\n      responseId: null,\n      ongoingToolCalls: {},\n      hasToolCalls: false,\n      isFirstChunk: true,\n      hasTextStarted: false,\n      hasReasoningStarted: false,\n      textEnded: false,\n      reasoningEnded: false,\n      textId: generateId(),\n    };\n  }\n\n  private processChunk(\n    chunk: ParseResult<z.infer<typeof baseOllamaResponseSchema>>,\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n    options: any,\n  ) {\n    if ((options as any)?.includeRawChunks) {\n      controller.enqueue({ type: \"raw\", rawValue: (chunk as any).rawValue });\n    }\n\n    const values = extractOllamaResponseObjectsFromChunk(chunk);\n\n    if (values.length === 0) {\n      if (!chunk.success) {\n        this.state.finishReason = \"error\";\n        controller.enqueue({ type: \"error\", error: chunk.error });\n      }\n      return;\n    }\n\n    for (const value of values) {\n      this.processResponseValue(value, controller);\n    }\n  }\n\n  private processResponseValue(\n    value: OllamaResponse,\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    // Handle error-like chunks\n    if ((value as any) && typeof (value as any) === \"object\" && \"error\" in (value as any)) {\n      this.state.finishReason = \"error\";\n      controller.enqueue({ type: \"error\", error: (value as any).error });\n      return;\n    }\n\n    if (this.state.isFirstChunk) {\n      this.state.isFirstChunk = false;\n      controller.enqueue({\n        type: \"response-metadata\",\n        ...getResponseMetadata(value as any),\n      });\n    }\n\n    if (value.done) {\n      this.handleDoneChunk(value, controller);\n    }\n\n    const delta = value?.message;\n    if (delta) {\n      this.processDelta(delta, controller);\n    }\n  }\n\n  private handleDoneChunk(\n    value: OllamaResponse,\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    this.state.finishReason = mapOllamaFinishReason(value.done_reason);\n    this.state.usage = {\n      inputTokens: value.prompt_eval_count || 0,\n      outputTokens: value.eval_count ?? undefined,\n      totalTokens: (value.prompt_eval_count ?? 0) + (value.eval_count ?? 0),\n    };\n\n    // Close any started streams\n    if (this.state.hasTextStarted && !this.state.textEnded) {\n      controller.enqueue({ type: \"text-end\", id: this.state.textId });\n      this.state.textEnded = true;\n    }\n    if (this.state.hasReasoningStarted && !this.state.reasoningEnded) {\n      controller.enqueue({ type: \"reasoning-end\", id: \"0\" });\n      this.state.reasoningEnded = true;\n    }\n  }\n\n  private processDelta(\n    delta: OllamaResponse[\"message\"],\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    this.processTextContent(delta, controller);\n    this.processThinking(delta, controller);\n    this.processToolCalls(delta, controller);\n  }\n\n  private processTextContent(\n    delta: OllamaResponse[\"message\"],\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    if (delta?.content != null) {\n      if (!this.state.hasTextStarted) {\n        controller.enqueue({ type: \"text-start\", id: this.state.textId });\n        this.state.hasTextStarted = true;\n      }\n      controller.enqueue({\n        type: \"text-delta\",\n        id: this.state.textId,\n        delta: delta.content,\n      });\n    }\n  }\n\n  private processThinking(\n    delta: OllamaResponse[\"message\"],\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    if (delta?.thinking) {\n      if (!this.state.hasReasoningStarted) {\n        controller.enqueue({ type: \"reasoning-start\", id: \"0\" });\n        this.state.hasReasoningStarted = true;\n      }\n      controller.enqueue({\n        type: \"reasoning-delta\",\n        id: \"0\",\n        delta: delta.thinking,\n      });\n    }\n  }\n\n  private processToolCalls(\n    delta: OllamaResponse[\"message\"],\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    for (const toolCall of delta.tool_calls ?? []) {\n      if (toolCall.function?.name == null) {\n        throw new InvalidResponseDataError({\n          data: toolCall,\n          message: `Expected 'function.name' to be a string.`,\n        });\n      }\n\n      if (\n        toolCall.function?.name != null &&\n        toolCall.function?.arguments != null\n      ) {\n        this.emitToolCall(toolCall, controller);\n      }\n    }\n  }\n\n  private emitToolCall(\n    toolCall: NonNullable<OllamaResponse[\"message\"][\"tool_calls\"]>[0],\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    const id = toolCall.id ?? (this.config.generateId?.() ?? generateId());\n\n    controller.enqueue({\n      type: \"tool-input-start\",\n      id: id,\n      toolName: toolCall.function.name,\n    });\n\n    controller.enqueue({\n      type: \"tool-input-delta\",\n      id: id,\n      delta: JSON.stringify(toolCall.function.arguments),\n    });\n\n    controller.enqueue({\n      type: \"tool-input-end\",\n      id: id,\n    });\n\n    controller.enqueue({\n      type: \"tool-call\",\n      toolCallId: id,\n      toolName: toolCall.function.name,\n      input: JSON.stringify(toolCall.function.arguments),\n    });\n\n    this.state.hasToolCalls = true;\n  }\n\n  private finalizeStream(\n    controller: TransformStreamDefaultController<LanguageModelV2StreamPart>,\n  ) {\n    // Ensure any started segments are properly closed\n    if (this.state.hasTextStarted && !this.state.textEnded) {\n      controller.enqueue({ type: \"text-end\", id: \"0\" });\n    }\n    if (this.state.hasReasoningStarted && !this.state.reasoningEnded) {\n      controller.enqueue({ type: \"reasoning-end\", id: \"0\" });\n    }\n\n    controller.enqueue({\n      type: \"finish\",\n      finishReason: this.state.finishReason,\n      usage: this.state.usage,\n      providerMetadata: {\n        ollama: {\n          responseId: this.state.responseId,\n        },\n      },\n    });\n  }\n} "],"mappings":";AAAA;AAAA,EAIE;AAAA,OACK;AACP;AAAA,EAEE;AAAA,OACK;;;ACTP;AAAA,EACE;AAAA,EACA;AAAA,EACA;AAAA,EAEA;AAAA,EAEA;AAAA,OACK;AACP,SAAS,KAAAA,UAAS;;;ACTlB,SAAS,SAAS;AAClB,SAAS,sCAAsC;AAExC,IAAM,wBAAwB,EAAE,OAAO;AAAA,EAC5C,OAAO,EAAE,OAAO;AAAA,IACd,SAAS,EAAE,OAAO;AAAA;AAAA;AAAA;AAAA,IAKlB,MAAM,EAAE,OAAO,EAAE,QAAQ;AAAA,IACzB,OAAO,EAAE,IAAI,EAAE,QAAQ;AAAA,IACvB,MAAM,EAAE,MAAM,CAAC,EAAE,OAAO,GAAG,EAAE,OAAO,CAAC,CAAC,EAAE,QAAQ;AAAA,EAClD,CAAC;AACH,CAAC;AAIM,IAAM,8BAA8B,+BAA+B;AAAA,EACxE,aAAa;AAAA,EACb,gBAAgB,UAAQ,KAAK,MAAM;AACrC,CAAC;;;ACrBD;AAAA,EACE;AAAA,EAEA;AAAA,OACK;AAEA,SAAS,gCAAgC;AAAA,EAC9C;AAAA,EACA,OAAO;AAAA,EACP,YAAY;AACd,GAOE;AAEA,MAAI,OAAO;AAGX,MAAI,OAAO,CAAC,EAAE,SAAS,UAAU;AAC/B,YAAQ,GAAG,OAAO,CAAC,EAAE,OAAO;AAAA;AAAA;AAC5B,aAAS,OAAO,MAAM,CAAC;AAAA,EACzB;AAEA,aAAW,EAAE,MAAM,QAAQ,KAAK,QAAQ;AACtC,YAAQ,MAAM;AAAA,MACZ,KAAK,UAAU;AACb,cAAM,IAAI,mBAAmB;AAAA,UAC3B,SAAS;AAAA,UACT;AAAA,QACF,CAAC;AAAA,MACH;AAAA,MAEA,KAAK,QAAQ;AACX,cAAM,cAAc,QACjB,IAAI,UAAQ;AACX,kBAAQ,KAAK,MAAM;AAAA,YACjB,KAAK,QAAQ;AACX,qBAAO,KAAK;AAAA,YACd;AAAA,UACF;AAAA,QACF,CAAC,EACA,OAAO,OAAO,EACd,KAAK,EAAE;AAEV,gBAAQ,GAAG,IAAI;AAAA,EAAM,WAAW;AAAA;AAAA;AAChC;AAAA,MACF;AAAA,MAEA,KAAK,aAAa;AAChB,cAAM,mBAAmB,QACtB,IAAI,UAAQ;AACX,kBAAQ,KAAK,MAAM;AAAA,YACjB,KAAK,QAAQ;AACX,qBAAO,KAAK;AAAA,YACd;AAAA,YACA,KAAK,aAAa;AAChB,oBAAM,IAAI,8BAA8B;AAAA,gBACtC,eAAe;AAAA,cACjB,CAAC;AAAA,YACH;AAAA,UACF;AAAA,QACF,CAAC,EACA,KAAK,EAAE;AAEV,gBAAQ,GAAG,SAAS;AAAA,EAAM,gBAAgB;AAAA;AAAA;AAC1C;AAAA,MACF;AAAA,MAEA,KAAK,QAAQ;AACX,cAAM,IAAI,8BAA8B;AAAA,UACtC,eAAe;AAAA,QACjB,CAAC;AAAA,MACH;AAAA,MAEA,SAAS;AACP,cAAM,mBAA0B;AAChC,cAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,MACzD;AAAA,IACF;AAAA,EACF;AAGA,UAAQ,GAAG,SAAS;AAAA;AAEpB,SAAO;AAAA,IACL,QAAQ;AAAA,IACR,eAAe,CAAC;AAAA,EAAK,IAAI,GAAG;AAAA,EAC9B;AACF;;;AC1FO,SAAS,sBACd,cAC6B;AAC7B,UAAQ,cAAc;AAAA,IACpB,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AAAA,IACL,KAAK;AACH,aAAO;AAAA,IACT;AACE,aAAO;AAAA,EACX;AACF;;;AClBO,SAAS,oBAAoB;AAAA,EAClC;AAAA,EACA;AACF,GAGG;AACD,SAAO;AAAA,IACL,IAAI;AAAA,IACJ,SAAS,wBAAS;AAAA,IAClB,WAAW,cAAc,OAAO,IAAI,KAAK,UAAU,IAAI;AAAA,EACzD;AACF;;;AJgBA,IAAM,kCAAkCC,GAAE,OAAO;AAAA,EAC/C,OAAOA,GAAE,QAAQ,EAAE,SAAS;AAAA,EAC5B,MAAMA,GAAE,OAAO,EAAE,SAAS;AAAA,EAC1B,QAAQA,GAAE,OAAO,EAAE,SAAS;AAAA,EAC5B,MAAMA,GAAE,QAAQ,EAAE,SAAS;AAC7B,CAAC;AAWM,IAAM,gCAAN,MAA+D;AAAA,EAQpE,YACE,SACA,UACA,QACA;AAXF,SAAS,uBAAuB;AAqBhC,SAAS,gBAA0C;AAAA;AAAA,IAEnD;AAXE,SAAK,UAAU;AACf,SAAK,WAAW;AAChB,SAAK,SAAS;AAAA,EAChB;AAAA,EAEA,IAAI,WAAmB;AACrB,WAAO,KAAK,OAAO;AAAA,EACrB;AAAA,EAMA,MAAc,QAAQ;AAAA,IACpB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA,eAAe;AAAA,IACf;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAAiD;AApFnD;AAqFI,UAAM,WAAyC,CAAC;AAGhD,UAAM,iBACH,WAAM,qBAAqB;AAAA,MAC1B,UAAU;AAAA,MACV;AAAA,MACA,QAAQ;AAAA,IACV,CAAC,MAJA,YAIM,CAAC;AAEV,QAAI,QAAQ,MAAM;AAChB,eAAS,KAAK;AAAA,QACZ,MAAM;AAAA,QACN,SAAS;AAAA,MACX,CAAC;AAAA,IACH;AAEA,QAAI,+BAAO,QAAQ;AACjB,eAAS,KAAK,EAAE,MAAM,uBAAuB,SAAS,QAAQ,CAAC;AAAA,IACjE;AAEA,QAAI,cAAc,MAAM;AACtB,eAAS,KAAK,EAAE,MAAM,uBAAuB,SAAS,aAAa,CAAC;AAAA,IACtE;AAEA,QAAI,kBAAkB,QAAQ,eAAe,SAAS,QAAQ;AAC5D,eAAS,KAAK;AAAA,QACZ,MAAM;AAAA,QACN,SAAS;AAAA,QACT,SAAS;AAAA,MACX,CAAC;AAAA,IACH;AAEA,UAAM,EAAE,QAAQ,kBAAkB,cAAc,IAC9C,gCAAgC,EAAE,OAAO,CAAC;AAE5C,UAAM,OAAO,CAAC,GAAI,wCAAiB,CAAC,GAAI,GAAI,gDAAqB,CAAC,CAAE;AAEpE,WAAO;AAAA,MACL,MAAM;AAAA;AAAA,QAEJ,OAAO,KAAK;AAAA;AAAA,QAGZ,MAAM,cAAc;AAAA,QACpB,OAAO,cAAc;AAAA;AAAA,QAGrB,YAAY;AAAA,QACZ;AAAA,QACA,OAAO;AAAA,QACP,mBAAmB;AAAA,QACnB,kBAAkB;AAAA,QAClB;AAAA;AAAA,QAGA,QAAQ;AAAA;AAAA,QAGR,QAAQ,cAAc;AAAA,QACtB,MAAM,cAAc;AAAA,QACpB,QAAQ;AAAA;AAAA,MACV;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,WACJ,SAC6D;AA1JjE;AA2JI,UAAM,EAAE,MAAM,MAAM,SAAS,IAAI,MAAM,KAAK,QAAQ,OAAO;AAE3D,UAAM;AAAA,MACJ;AAAA,MACA,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,IAAI,MAAM,cAAc;AAAA,MACtB,KAAK,KAAK,OAAO,IAAI;AAAA,QACnB,MAAM;AAAA,QACN,SAAS,KAAK;AAAA,MAChB,CAAC;AAAA,MACD,SAAS,eAAe,KAAK,OAAO,QAAQ,GAAG,QAAQ,OAAO;AAAA,MAC9D,MAAM,EAAE,GAAG,MAAM,QAAQ,MAAM;AAAA,MAC/B,uBAAuB;AAAA,MACvB,2BAA2B;AAAA,QACzB;AAAA,MACF;AAAA,MACA,aAAa,QAAQ;AAAA,MACrB,OAAO,KAAK,OAAO;AAAA,IACrB,CAAC;AAED,UAAM,EAAE,QAAQ,WAAW,GAAG,YAAY,IAAI;AAE9C,UAAM,mBAA6C,EAAE,QAAQ,CAAC,EAAE;AAEhE,WAAO;AAAA,MACL,SAAS;AAAA,QACP;AAAA,UACE,MAAM;AAAA,UACN,MAAM,SAAS;AAAA,QACjB;AAAA,MACF;AAAA,MACA,OAAO;AAAA,QACL,cAAa,cAAS,sBAAT,YAA8B;AAAA,QAC3C,eAAc,cAAS,eAAT,YAAuB;AAAA,QACrC,eAAc,cAAS,sBAAT,YAA8B,OAAM,cAAS,eAAT,YAAuB;AAAA,MAC3E;AAAA,MACA,cAAc,sBAAsB,MAAM;AAAA,MAC1C,SAAS,EAAE,MAAM,KAAK,UAAU,IAAI,EAAE;AAAA,MACtC,UAAU;AAAA,QACR,GAAG,oBAAoB,QAAQ;AAAA,QAC/B,SAAS;AAAA,QACT,MAAM;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,SACJ,SAC2D;AAC3D,UAAM,EAAE,MAAM,SAAS,IAAI,MAAM,KAAK,QAAQ,OAAO;AAErD,UAAM,OAAO;AAAA,MACX,GAAG;AAAA,MACH,QAAQ;AAAA,IACV;AAEA,UAAM,EAAE,iBAAiB,OAAO,SAAS,IAAI,MAAM,cAAc;AAAA,MAC/D,KAAK,KAAK,OAAO,IAAI;AAAA,QACnB,MAAM;AAAA,QACN,SAAS,KAAK;AAAA,MAChB,CAAC;AAAA,MACD,SAAS,eAAe,KAAK,OAAO,QAAQ,GAAG,QAAQ,OAAO;AAAA,MAC9D;AAAA,MACA,uBAAuB;AAAA,MACvB,2BAA2B;AAAA,QACzB;AAAA,MACF;AAAA,MACA,aAAa,QAAQ;AAAA,MACrB,OAAO,KAAK,OAAO;AAAA,IACrB,CAAC;AAED,UAAM,EAAE,QAAQ,WAAW,GAAG,YAAY,IAAI;AAE9C,QAAI,eAA4C;AAChD,QAAI,QAA8B;AAAA,MAChC,aAAa;AAAA,MACb,cAAc;AAAA,MACd,aAAa;AAAA,IACf;AACA,QAAI,eAAe;AAEnB,WAAO;AAAA,MACL,QAAQ,SAAS;AAAA,QACf,IAAI,gBAGF;AAAA,UACA,UAAU,OAAO,YAAY;AAE3B,gBAAI,CAAC,MAAM,SAAS;AAClB,6BAAe;AACf,yBAAW,QAAQ,EAAE,MAAM,SAAS,OAAQ,MAAc,MAAM,CAAC;AACjE;AAAA,YACF;AAEA,kBAAM,QAAQ,MAAM;AAGpB,gBAAI,WAAW,OAAO;AACpB,6BAAe;AACf,yBAAW,QAAQ,EAAE,MAAM,SAAS,OAAO,MAAM,MAAM,CAAC;AACxD;AAAA,YACF;AAEA,gBAAI,cAAc;AAChB,6BAAe;AAEf,yBAAW,QAAQ;AAAA,gBACjB,MAAM;AAAA,gBACN,GAAG,oBAAoB,KAAK;AAAA,cAC9B,CAAC;AAAA,YACH;AAEA,gBAAI,MAAM,MAAM;AACd,6BAAe,sBAAsB,MAAM;AAAA,YAC7C;AAEA,gBAAI,MAAM,YAAY,MAAM;AAC1B,yBAAW,QAAQ;AAAA,gBACjB,MAAM;AAAA,gBACN,IAAI;AAAA,gBACJ,OAAO,MAAM;AAAA,cACf,CAAC;AAAA,YACH;AAAA,UACF;AAAA,UAEA,MAAM,YAAY;AAChB,uBAAW,QAAQ;AAAA,cACjB,MAAM;AAAA,cACN;AAAA,cACA;AAAA,YACF,CAAC;AAAA,UACH;AAAA,QACF,CAAC;AAAA,MACH;AAAA,MACA,SAAS,EAAE,MAAM,KAAK,UAAU,IAAI,EAAE;AAAA,MACtC,UAAU,EAAE,SAAS,gBAAgB;AAAA,IACvC;AAAA,EACF;AACF;AAEA,IAAM,2BAA2BA,GAAE,OAAO;AAAA,EACxC,OAAOA,GAAE,OAAO;AAAA,EAChB,YAAYA,GAAE,OAAO;AAAA,EACrB,UAAUA,GAAE,OAAO;AAAA,EACnB,MAAMA,GAAE,QAAQ;AAAA,EAChB,SAASA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,EAE3B,YAAYA,GAAE,OAAO,EAAE,SAAS;AAAA,EAChC,eAAeA,GAAE,OAAO,EAAE,SAAS;AAAA,EAEnC,eAAeA,GAAE,OAAO,EAAE,SAAS;AAAA,EACnC,gBAAgBA,GAAE,OAAO,EAAE,SAAS;AAAA,EAEpC,mBAAmBA,GAAE,OAAO,EAAE,SAAS;AAAA,EACvC,sBAAsBA,GAAE,OAAO,EAAE,SAAS;AAC5C,CAAC;;;AK1TD;AAAA,EAEE;AAAA,OACK;AACP;AAAA,EACE,kBAAAC;AAAA,EACA,6BAAAC;AAAA,EACA,wBAAAC;AAAA,EACA,iBAAAC;AAAA,OACK;AACP,SAAS,KAAAC,UAAS;AAQlB,IAAM,iCAAiCC,GAAE,OAAO;AAAA,EAC9C,YAAYA,GAAE,OAAO,EAAE,SAAS;AAAA,EAChC,UAAUA,GAAE,QAAQ,EAAE,SAAS;AAAA,EAC/B,WAAWA,GAAE,OAAO,EAAE,SAAS;AACjC,CAAC;AAIM,IAAM,uBAAN,MAA+D;AAAA,EAmBpE,YACE,SACA,UACA,QACA;AAtBF,SAAS,uBAAuB;AAuB9B,SAAK,UAAU;AACf,SAAK,WAAW;AAChB,SAAK,SAAS;AAAA,EAChB;AAAA,EApBA,IAAI,WAAmB;AACrB,WAAO,KAAK,OAAO;AAAA,EACrB;AAAA,EAEA,IAAI,uBAA+B;AArCrC;AAsCI,YAAO,UAAK,SAAS,yBAAd,YAAsC;AAAA,EAC/C;AAAA,EAEA,IAAI,wBAAiC;AAzCvC;AA0CI,YAAO,UAAK,SAAS,0BAAd,YAAuC;AAAA,EAChD;AAAA,EAYA,MAAc,QAAQ;AAAA,IACpB;AAAA,IACA;AAAA,EACF,GAAuD;AA1DzD;AA4DI,UAAM,iBACH,WAAMC,sBAAqB;AAAA,MAC1B,UAAU;AAAA,MACV;AAAA,MACA,QAAQ;AAAA,IACV,CAAC,MAJA,YAIM,CAAC;AAEV,WAAO;AAAA,MACL,MAAM;AAAA;AAAA,QAEJ,OAAO,KAAK;AAAA,QACZ,OAAO;AAAA;AAAA,QAGP,aAAY,mBAAc,eAAd,YAA4B,KAAK,SAAS;AAAA,QACtD,UAAU,cAAc;AAAA,QACxB,YAAY,cAAc;AAAA,MAC5B;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,QAAQ;AAAA,IACZ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAEE;AACA,QAAI,OAAO,SAAS,KAAK,sBAAsB;AAC7C,YAAM,IAAI,mCAAmC;AAAA,QAC3C,UAAU,KAAK;AAAA,QACf,SAAS,KAAK;AAAA,QACd,sBAAsB,KAAK;AAAA,QAC3B;AAAA,MACF,CAAC;AAAA,IACH;AAEA,UAAM,EAAE,MAAM,KAAK,IAAI,MAAM,KAAK,QAAQ,EAAC,QAAQ,gBAAe,CAAC;AAEnE,UAAM;AAAA,MACJ;AAAA,MACA,OAAO;AAAA,MACP;AAAA,IACF,IAAI,MAAMC,eAAc;AAAA,MACtB,KAAK,KAAK,OAAO,IAAI;AAAA,QACnB,MAAM;AAAA,QACN,SAAS,KAAK;AAAA,MAChB,CAAC;AAAA,MACD,SAASC,gBAAe,KAAK,OAAO,QAAQ,GAAG,OAAO;AAAA,MACtD,MAAM,EAAE,GAAG,KAAK;AAAA,MAChB,uBAAuB;AAAA,MACvB,2BAA2BC;AAAA,QACzB;AAAA,MACF;AAAA,MACA;AAAA,MACA,OAAO,KAAK,OAAO;AAAA,IACrB,CAAC;AAED,WAAO;AAAA,MACL,YAAY,SAAS,WAAW,IAAI,CAAC,SAAS,IAAI;AAAA,MAClD,OAAO,EAAE,QAAQ,SAAS,kBAAkB;AAAA,MAC5C,UAAU,EAAE,SAAS,iBAAiB,MAAM,SAAS;AAAA,IACvD;AAAA,EACF;AACF;AAIA,IAAM,oCAAoCJ,GAAE,OAAO;AAAA,EACjD,OAAOA,GAAE,OAAO;AAAA,EAChB,YAAYA,GAAE,MAAMA,GAAE,MAAMA,GAAE,OAAO,CAAC,CAAC;AAAA,EACvC,gBAAgBA,GAAE,OAAO;AAAA,EACzB,eAAeA,GAAE,OAAO;AAAA,EACxB,mBAAmBA,GAAE,OAAO;AAC9B,CAAC;;;ACnID;AAAA,EACE,kBAAAK;AAAA,EACA,6BAAAC;AAAA,EACA,mCAAAC;AAAA,EACA,iBAAAC;AAAA,OACK;;;ACLP,SAAS,wBAAAC,6BAA4B;;;ACJrC;AAAA,EAGI,iCAAAC;AAAA,OACK;AAGA,SAAS,iCAAiC;AAAA,EAC/C;AAAA,EACA;AACF,GAME;AACA,QAAM,WAAkC,CAAC;AACzC,QAAM,WAA8C,CAAC;AAErD,aAAW,EAAE,MAAM,QAAQ,KAAK,QAAQ;AACtC,YAAQ,MAAM;AAAA,MACZ,KAAK,UAAU;AACb,gBAAQ,mBAAmB;AAAA,UACzB,KAAK,UAAU;AACb,qBAAS,KAAK,EAAE,MAAM,UAAU,QAAQ,CAAC;AACzC;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,qBAAS,KAAK,EAAE,MAAM,aAAa,QAAQ,CAAC;AAC5C;AAAA,UACF;AAAA,UACA,KAAK,UAAU;AACb,qBAAS,KAAK;AAAA,cACZ,MAAM;AAAA,cACN,SAAS;AAAA,YACX,CAAC;AACD;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI;AAAA,cACR,oCAAoC,gBAAgB;AAAA,YACtD;AAAA,UACF;AAAA,QACF;AACA;AAAA,MACF;AAAA,MAEA,KAAK,QAAQ;AACX,iBAAS,KAAK;AAAA,UACZ,MAAM;AAAA,UACN,SAAS,QAAQ,IAAI,CAAC,MAAM,UAAU;AApDlD;AAqDc,oBAAQ,KAAK,MAAM;AAAA,cACjB,KAAK,QAAQ;AACX,uBAAO,EAAE,MAAM,cAAc,MAAM,KAAK,KAAK;AAAA,cAC/C;AAAA,cACA,KAAK,QAAQ;AACX,oBAAI,KAAK,UAAU,WAAW,QAAQ,GAAG;AACvC,wBAAM,YACJ,KAAK,cAAc,YACf,eACA,KAAK;AAEX,yBAAO;AAAA,oBACL,MAAM;AAAA,oBACN,WACE,KAAK,gBAAgB,MACjB,KAAK,KAAK,SAAS,IACnB,QAAQ,SAAS,WAAW,KAAK,IAAI;AAAA;AAAA,oBAG3C,SAAQ,gBAAK,oBAAL,mBAAsB,WAAtB,mBAA8B;AAAA,kBACxC;AAAA,gBACF,WAAW,KAAK,cAAc,mBAAmB;AAC/C,sBAAI,KAAK,gBAAgB,KAAK;AAE5B,0BAAM,IAAIA,+BAA8B;AAAA,sBACtC,eAAe;AAAA,oBACjB,CAAC;AAAA,kBACH;AAEA,yBAAO;AAAA,oBACL,MAAM;AAAA,oBACN,WAAU,UAAK,aAAL,YAAiB,QAAQ,KAAK;AAAA,oBACxC,WAAW,+BAA+B,KAAK,IAAI;AAAA,kBACrD;AAAA,gBACF,OAAO;AACL,wBAAM,IAAIA,+BAA8B;AAAA,oBACtC,eAAe,wBAAwB,KAAK,SAAS;AAAA,kBACvD,CAAC;AAAA,gBACH;AAAA,cACF;AAAA,YACF;AAAA,UACF,CAAC;AAAA,QACH,CAAC;AAED;AAAA,MACF;AAAA,MAEA,KAAK,aAAa;AAChB,mBAAW,QAAQ,SAAS;AAC1B,kBAAQ,KAAK,MAAM;AAAA,YACjB,KAAK,QAAQ;AACX,uBAAS,KAAK;AAAA,gBACZ,MAAM;AAAA,gBACN,SAAS,CAAC,EAAE,MAAM,eAAe,MAAM,KAAK,KAAK,CAAC;AAAA,cACpD,CAAC;AACD;AAAA,YACF;AAAA,YACA,KAAK,aAAa;AAChB,kBAAI,KAAK,kBAAkB;AACzB;AAAA,cACF;AAEA,uBAAS,KAAK;AAAA,gBACZ,MAAM;AAAA,gBACN,SAAS,KAAK;AAAA,gBACd,MAAM,KAAK;AAAA,gBACX,WAAW,KAAK,UAAU,KAAK,KAAK;AAAA,cACtC,CAAC;AACD;AAAA,YACF;AAAA,YAEA,KAAK,eAAe;AAClB,uBAAS,KAAK;AAAA,gBACZ,MAAM;AAAA,gBACN,SAAS;AAAA,cACX,CAAC;AACD;AAAA,YACF;AAAA,UACF;AAAA,QACF;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,QAAQ;AACX,mBAAW,QAAQ,SAAS;AAC1B,gBAAM,SAAS,KAAK;AAEpB,cAAI;AACJ,kBAAQ,OAAO,MAAM;AAAA,YACnB,KAAK;AAAA,YACL,KAAK;AACH,6BAAe,OAAO;AACtB;AAAA,YACF,KAAK;AAAA,YACL,KAAK;AAAA,YACL,KAAK;AACH,6BAAe,KAAK,UAAU,OAAO,KAAK;AAC1C;AAAA,UACJ;AAEA,mBAAS,KAAK;AAAA,YACZ,MAAM;AAAA,YACN,SAAS,KAAK;AAAA,YACd,QAAQ;AAAA,UACV,CAAC;AAAA,QACH;AAEA;AAAA,MACF;AAAA,MAEA,SAAS;AACP,cAAM,mBAA0B;AAChC,cAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,MACzD;AAAA,IACF;AAAA,EACF;AAEA,SAAO,EAAE,UAAU,SAAS;AAC9B;;;ACtKK,SAAS,4BAA4B;AAAA,EAC1C;AAAA,EACA,oBAAoB;AACtB,GAGqB;AACnB,QAAM,WAA6B,CAAC;AAEpC,aAAW,EAAE,MAAM,QAAQ,KAAK,QAAQ;AACtC,YAAQ,MAAM;AAAA,MACZ,KAAK,UAAU;AACb,gBAAQ,mBAAmB;AAAA,UACzB,KAAK,UAAU;AACb,qBAAS,KAAK,EAAE,MAAM,UAAU,QAAQ,CAAC;AACzC;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,qBAAS,KAAK,EAAE,MAAM,aAAa,QAAQ,CAAC;AAC5C;AAAA,UACF;AAAA,UACA,KAAK,UAAU;AACb;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI;AAAA,cACR,oCAAoC,gBAAgB;AAAA,YACtD;AAAA,UACF;AAAA,QACF;AACA;AAAA,MACF;AAAA,MAEA,KAAK,QAAQ;AACX,YAAI,QAAQ,WAAW,KAAK,QAAQ,CAAC,EAAE,SAAS,QAAQ;AACtD,mBAAS,KAAK,EAAE,MAAM,QAAQ,SAAS,QAAQ,CAAC,EAAE,KAAK,CAAC;AACxD;AAAA,QACF;AAEA,cAAM,WAAW,QAAQ,OAAO,CAAC,SAAS,KAAK,SAAS,MAAM,EAAE,IAAI,CAAC,SAAS,KAAK,IAAI,EAAE,KAAK,EAAE;AAChG,cAAM,SAAS,QACZ,OAAO,CAAC,SAAS,KAAK,SAAS,UAAU,KAAK,UAAU,WAAW,QAAQ,CAAC,EAC5E,IAAI,CAAC,SAAU,KAAiC,IAAI;AAEvD,iBAAS,KAAK;AAAA,UACZ,MAAM;AAAA,UACN,SAAS,SAAS,SAAS,IAAI,WAAW,CAAC;AAAA,UAC3C,QAAQ,OAAO,SAAS,IAAI,SAAS;AAAA,QACvC,CAAC;AAED;AAAA,MACF;AAAA,MAEA,KAAK,aAAa;AAChB,YAAI,OAAO;AACX,YAAI,WAAW;AACf,cAAM,YAID,CAAC;AAEN,mBAAW,QAAQ,SAAS;AAC1B,kBAAQ,KAAK,MAAM;AAAA,YACjB,KAAK,QAAQ;AACX,sBAAQ,KAAK;AACb;AAAA,YACF;AAAA,YACA,KAAK,aAAa;AAChB,wBAAU,KAAK;AAAA,gBACb,IAAI,KAAK;AAAA,gBACT,MAAM;AAAA,gBACN,UAAU;AAAA,kBACR,MAAM,KAAK;AAAA,kBACX,WAAW,KAAK;AAAA,gBAClB;AAAA,cACF,CAAC;AACD;AAAA,YACF;AAAA,YACA,KAAK,aAAa;AAChB,0BAAY,KAAK;AACjB;AAAA,YACF;AAAA,YACA,SAAS;AACP,oBAAM,IAAI,MAAM,qBAAqB,IAAI,EAAE;AAAA,YAC7C;AAAA,UACF;AAAA,QACF;AAEA,iBAAS,KAAK;AAAA,UACZ,MAAM;AAAA,UACN,SAAS;AAAA,UACT,GAAI,YAAY,EAAE,SAAS;AAAA,UAC3B,YAAY,UAAU,SAAS,IAAI,YAAY;AAAA,QACjD,CAAC;AAED;AAAA,MACF;AAAA,MAEA,KAAK,QAAQ;AACX,mBAAW,gBAAgB,SAAS;AAClC,gBAAM,SAAS,aAAa;AAE5B,cAAI;AACJ,kBAAQ,OAAO,MAAM;AAAA,YACnB,KAAK;AAAA,YACL,KAAK;AACH,6BAAe,OAAO;AACtB;AAAA,YACF,KAAK;AAAA,YACL,KAAK;AAAA,YACL,KAAK;AACH,6BAAe,KAAK,UAAU,OAAO,KAAK;AAC1C;AAAA,UACJ;AAEA,mBAAS,KAAK;AAAA,YACZ,MAAM;AAAA,YACN,cAAc,aAAa;AAAA,YAC3B,SAAS;AAAA,UACX,CAAC;AAAA,QACH;AACA;AAAA,MACF;AAAA,MAEA,SAAS;AACP,cAAM,mBAA0B;AAChC,cAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,MACzD;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;;;AC5IA;AAAA,EAGE,iCAAAC;AAAA,OACK;AAGA,SAAS,sBAAsB;AAAA,EACpC;AAAA,EACA;AACF,GAYE;AAEA,WAAQ,+BAAO,UAAS,QAAQ;AAEhC,QAAM,eAA6C,CAAC;AAEpD,MAAI,SAAS,MAAM;AACjB,WAAO,EAAE,OAAO,QAAW,YAAY,QAAW,aAAa;AAAA,EACjE;AAEA,QAAM,cAA0C,CAAC;AAEjD,aAAW,QAAQ,OAAO;AACxB,YAAQ,KAAK,MAAM;AAAA,MACjB,KAAK,YAAY;AAEf,YAAI,aAAa,KAAK;AACtB,YAAG,CAAC,YAAW;AACb,uBAAa;AAAA,YACX,MAAM;AAAA,YACN,YAAY,CAAC;AAAA,YACb,UAAU,CAAC;AAAA,UACb;AAAA,QACF,WAEE,cACA,OAAO,eAAe,YACtB,WAAW,SAAS,YACpB,WAAW,cACX,OAAO,KAAK,WAAW,UAAU,EAAE,WAAW,GAC9C;AAEA,uBAAa;AAAA,YACX,GAAG;AAAA,YACH,YAAY,CAAC;AAAA,YACb,UAAU,CAAC;AAAA,UACb;AAAA,QACF;AAEA,oBAAY,KAAK;AAAA,UACf,MAAM;AAAA,UACN,UAAU;AAAA,YACR,MAAM,KAAK;AAAA,YACX,aAAa,KAAK;AAAA,YAClB;AAAA,UACF;AAAA,QACF,CAAC;AACD;AAAA,MACF;AAAA,MACA;AACE,qBAAa,KAAK,EAAE,MAAM,oBAAoB,KAAK,CAAC;AACpD;AAAA,IACJ;AAAA,EACF;AAEA,MAAI,cAAc,MAAM;AACtB,WAAO,EAAE,OAAO,aAAa,YAAY,QAAW,aAAa;AAAA,EACnE;AAEA,QAAM,OAAO,WAAW;AAExB,UAAQ,MAAM;AAAA,IACZ,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AACH,aAAO,EAAE,OAAO,aAAa,YAAY,MAAM,aAAa;AAAA,IAC9D,KAAK;AACH,aAAO;AAAA,QACL,OAAO;AAAA,QACP,YACE,WAAW,YAAY,uBACnB,EAAE,MAAM,qBAAqB,IAC7B,EAAE,MAAM,YAAY,MAAM,WAAW,SAAS;AAAA,QACpD;AAAA,MACF;AAAA,IACF,SAAS;AACP,YAAM,mBAA0B;AAChC,YAAM,IAAIA,+BAA8B;AAAA,QACtC,eAAe,qBAAqB,gBAAgB;AAAA,MACtD,CAAC;AAAA,IACH;AAAA,EACF;AACF;;;ACxGA,SAAS,KAAAC,UAAS;AAqNX,IAAM,wBAAwBA,GAAE,OAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQ5C,OAAOA,GAAE,QAAQ,EAAE,SAAS;AAAA,EAC5B,SAASA,GAAE,OAAO;AAAA,IAChB,SAASA,GAAE,OAAO,EAAE,SAAS;AAAA,IAC7B,eAAeA,GAAE,OAAO,EAAE,SAAS;AAAA,IACnC,gBAAgBA,GAAE,OAAO,EAAE,SAAS;AAAA,IACpC,aAAaA,GAAE,OAAO,EAAE,SAAS;AAAA,IACjC,MAAMA,GAAE,OAAO,EAAE,SAAS;AAAA,IAC1B,MAAMA,GAAE,MAAMA,GAAE,OAAO,CAAC,EAAE,SAAS;AAAA,IACnC,aAAaA,GAAE,OAAO,EAAE,SAAS;AAAA,IACjC,OAAOA,GAAE,OAAO,EAAE,SAAS;AAAA,IAC3B,OAAOA,GAAE,OAAO,EAAE,SAAS;AAAA,IAC3B,OAAOA,GAAE,OAAO,EAAE,SAAS;AAAA,EAC7B,CAAC,EAAE,SAAS;AAGd,CAAC;;;AJ3LM,IAAM,uBAAN,MAA2B;AAAA,EAChC,MAAM,aAAa;AAAA,IACjB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAAyD;AACvD,UAAM,WAAW,KAAK,mCAAmC;AAAA,MACvD;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,UAAM,EAAE,UAAU,UAAU,gBAAgB,IAC1C,iCAAiC;AAAA,MAC/B;AAAA,MACA,mBAAmB;AAAA,IACrB,CAAC;AAEH,aAAS,KAAK,GAAG,eAAe;AAEhC,UAAM,gBAAgB,MAAM,KAAK,qBAAqB,eAAe;AAErE,UAAM,WAAW,KAAK,cAAc;AAAA,MAClC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAED,UAAM,EAAE,OAAO,aAAa,YAAY,kBAAkB,aAAa,IACrE,sBAAsB;AAAA,MACpB;AAAA,MACA;AAAA,IACF,CAAC;AAEH,WAAO;AAAA,MACL,MAAM;AAAA,QACJ,GAAG;AAAA,QACH,OAAO;AAAA,QACP,aAAa;AAAA,MACf;AAAA,MACA,UAAU,CAAC,GAAG,UAAU,GAAG,YAAY;AAAA,IACzC;AAAA,EACF;AAAA,EAEQ,mCAAmC;AAAA,IACzC;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAMiC;AAC/B,UAAM,WAAyC,CAAC;AAEhD,UAAM,sBAAsB;AAAA,MAC1B,EAAE,OAAO,MAAM,MAAM,OAAO;AAAA,MAC5B,EAAE,OAAO,MAAM,MAAM,OAAO;AAAA,MAC5B,EAAE,OAAO,iBAAiB,MAAM,kBAAkB;AAAA,MAClD,EAAE,OAAO,kBAAkB,MAAM,mBAAmB;AAAA,MACpD,EAAE,OAAO,eAAe,MAAM,gBAAgB;AAAA,IAChD;AAEA,eAAW,EAAE,OAAO,KAAK,KAAK,qBAAqB;AACjD,UAAI,SAAS,MAAM;AACjB,iBAAS,KAAK,EAAE,MAAM,uBAAuB,SAAS,KAAK,CAAC;AAAA,MAC9D;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA,EAEA,MAAc,qBAAqB,iBAAsE;AACvG,UAAM,SAAS,MAAMC,sBAAqB;AAAA,MACxC,UAAU;AAAA,MACV;AAAA,MACA,QAAQ;AAAA,IACV,CAAC;AACD,WAAO,0BAAU;AAAA,EACnB;AAAA,EAEQ,cAAc;AAAA,IACpB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAQG;AAvKL;AAwKI,WAAO;AAAA,MACL,OAAO;AAAA,MACP,UAAU,4BAA4B;AAAA,QACpC;AAAA,QACA,mBAAmB;AAAA,MACrB,CAAC;AAAA,MACD;AAAA,MACA,OAAO;AAAA,MACP,mBAAmB;AAAA,MAEnB,IAAI,iDAAgB,UAAS,UAAU;AAAA,QACrC,QAAQ,eAAe,UAAU,OAAO,eAAe,SAAS;AAAA,MAClE;AAAA,MAEA,QAAO,oDAAe,UAAf,YAAwB;AAAA,MAC/B,UAAS,oDAAe,YAAf,YAAyB;AAAA,IACpC;AAAA,EACF;AACF;;;AKpLA,SAAS,cAAAC,mBAAkB;AAC3B,SAAS,KAAAC,UAAS;AAIX,IAAMC,4BAA2BC,GAAE,OAAO;AAAA,EAC/C,OAAOA,GAAE,OAAO;AAAA,EAChB,YAAYA,GAAE,OAAO;AAAA,EACrB,MAAMA,GAAE,QAAQ;AAAA,EAChB,SAASA,GAAE,OAAO;AAAA,IAChB,SAASA,GAAE,OAAO;AAAA,IAClB,MAAMA,GAAE,OAAO;AAAA,IACf,UAAUA,GAAE,OAAO,EAAE,SAAS;AAAA,IAC9B,YAAYA,GACT;AAAA,MACCA,GAAE,OAAO;AAAA,QACP,UAAUA,GAAE,OAAO;AAAA,UACjB,MAAMA,GAAE,OAAO;AAAA,UACf,WAAWA,GAAE,OAAOA,GAAE,OAAO,GAAGA,GAAE,IAAI,CAAC;AAAA,QACzC,CAAC;AAAA,QACD,IAAIA,GAAE,OAAO,EAAE,SAAS;AAAA,MAC1B,CAAC;AAAA,IACH,EACC,SAAS,EACT,SAAS;AAAA,EACd,CAAC;AAAA,EAED,aAAaA,GAAE,OAAO,EAAE,SAAS;AAAA,EACjC,YAAYA,GAAE,OAAO,EAAE,SAAS;AAAA,EAChC,eAAeA,GAAE,OAAO,EAAE,SAAS;AAAA,EACnC,eAAeA,GAAE,OAAO,EAAE,SAAS;AAAA,EACnC,mBAAmBA,GAAE,OAAO,EAAE,SAAS;AAAA,EACvC,sBAAsBA,GAAE,OAAO,EAAE,SAAS;AAAA,EAC1C,gBAAgBA,GAAE,OAAO,EAAE,SAAS;AACtC,CAAC;AAIM,IAAM,0BAAN,MAA8B;AAAA,EACnC,YAAoB,QAAsB;AAAtB;AAAA,EAAuB;AAAA,EAE3C,wBAAwB,UAKtB;AACA,UAAM,UAAU,KAAK,eAAe,QAAQ;AAC5C,UAAM,eAAe,sBAAsB,SAAS,WAAW;AAC/D,UAAM,QAAQ,KAAK,aAAa,QAAQ;AACxC,UAAM,mBAA6C,EAAE,QAAQ,CAAC,EAAE;AAEhE,WAAO;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,eAAe,UAAoD;AAlE7E;AAmEI,UAAM,UAAoC,CAAC;AAG3C,UAAM,OAAO,SAAS,QAAQ;AAC9B,QAAI,QAAQ,QAAQ,KAAK,SAAS,GAAG;AACnC,cAAQ,KAAK;AAAA,QACX,MAAM;AAAA,QACN;AAAA,MACF,CAAC;AAAA,IACH;AAGA,eAAW,aAAY,cAAS,QAAQ,eAAjB,YAA+B,CAAC,GAAG;AACxD,cAAQ,KAAK;AAAA,QACX,MAAM;AAAA,QACN,aAAY,cAAS,OAAT,aAAgB,sBAAK,QAAO,eAAZ,4CAA8BC,YAAW;AAAA,QACrE,UAAU,SAAS,SAAS;AAAA,QAC5B,OAAO,KAAK,UAAU,SAAS,SAAS,SAAS;AAAA,MACnD,CAAC;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAAA,EAEQ,aAAa,UAAgD;AA3FvE;AA4FI,WAAO;AAAA,MACL,cAAa,cAAS,sBAAT,YAA8B;AAAA,MAC3C,eAAc,cAAS,eAAT,YAAuB;AAAA,MACrC,eAAc,cAAS,sBAAT,YAA8B,OAAM,cAAS,eAAT,YAAuB;AAAA,MACzE,iBAAiB;AAAA;AAAA,MACjB,mBAAmB;AAAA,IACrB;AAAA,EACF;AACF;AAOO,SAAS,sCACd,OACkB;AA7GpB;AA8GE,MAAI,MAAM,SAAS;AACjB,WAAO,CAAC,MAAM,KAAK;AAAA,EACrB;AAEA,QAAM,UAA4B,CAAC;AACnC,QAAM,OAAO,WAAM,UAAN,mBAAqB;AAClC,MAAI,OAAO,QAAQ,YAAY,IAAI,WAAW,GAAG;AAC/C,WAAO;AAAA,EACT;AAEA,QAAM,QAAQ,IAAI,MAAM,OAAO;AAC/B,aAAW,QAAQ,OAAO;AACxB,UAAM,UAAU,KAAK,KAAK;AAC1B,QAAI,YAAY,GAAI;AACpB,QAAI;AACF,YAAM,SAAS,KAAK,MAAM,OAAO;AACjC,YAAM,YAAYF,0BAAyB,UAAU,MAAM;AAC3D,UAAI,UAAU,SAAS;AACrB,gBAAQ,KAAK,UAAU,IAAI;AAAA,MAC7B;AAAA,IACF,SAAQ;AAAA,IAER;AAAA,EACF;AAEA,SAAO;AACT;;;ACxIA;AAAA,EACE;AAAA,OAIK;AACP,SAAS,cAAAG,mBAA+B;AAyBjC,IAAM,wBAAN,MAA4B;AAAA,EAGjC,YAAoB,QAAsB;AAAtB;AAClB,SAAK,QAAQ,KAAK,gBAAgB;AAAA,EACpC;AAAA,EAEA,sBAAsB,UAAiB,SAGrC;AACA,WAAO,IAAI,gBAAgB;AAAA,MACzB,OAAO,CAAC,eAAe;AACrB,mBAAW,QAAQ,EAAE,MAAM,gBAAgB,SAAS,CAAC;AAAA,MACvD;AAAA,MAEA,WAAW,CAAC,OAAO,eAAe;AAChC,aAAK,aAAa,OAAO,YAAY,OAAO;AAAA,MAC9C;AAAA,MAEA,OAAO,CAAC,eAAe;AACrB,aAAK,eAAe,UAAU;AAAA,MAChC;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEQ,kBAA+B;AACrC,WAAO;AAAA,MACL,cAAc;AAAA,MACd,OAAO;AAAA,QACL,aAAa;AAAA,QACb,cAAc;AAAA,QACd,aAAa;AAAA,MACf;AAAA,MACA,YAAY;AAAA,MACZ,kBAAkB,CAAC;AAAA,MACnB,cAAc;AAAA,MACd,cAAc;AAAA,MACd,gBAAgB;AAAA,MAChB,qBAAqB;AAAA,MACrB,WAAW;AAAA,MACX,gBAAgB;AAAA,MAChB,QAAQC,YAAW;AAAA,IACrB;AAAA,EACF;AAAA,EAEQ,aACN,OACA,YACA,SACA;AACA,QAAK,mCAAiB,kBAAkB;AACtC,iBAAW,QAAQ,EAAE,MAAM,OAAO,UAAW,MAAc,SAAS,CAAC;AAAA,IACvE;AAEA,UAAM,SAAS,sCAAsC,KAAK;AAE1D,QAAI,OAAO,WAAW,GAAG;AACvB,UAAI,CAAC,MAAM,SAAS;AAClB,aAAK,MAAM,eAAe;AAC1B,mBAAW,QAAQ,EAAE,MAAM,SAAS,OAAO,MAAM,MAAM,CAAC;AAAA,MAC1D;AACA;AAAA,IACF;AAEA,eAAW,SAAS,QAAQ;AAC1B,WAAK,qBAAqB,OAAO,UAAU;AAAA,IAC7C;AAAA,EACF;AAAA,EAEQ,qBACN,OACA,YACA;AAEA,QAAK,SAAiB,OAAQ,UAAkB,YAAY,WAAY,OAAe;AACrF,WAAK,MAAM,eAAe;AAC1B,iBAAW,QAAQ,EAAE,MAAM,SAAS,OAAQ,MAAc,MAAM,CAAC;AACjE;AAAA,IACF;AAEA,QAAI,KAAK,MAAM,cAAc;AAC3B,WAAK,MAAM,eAAe;AAC1B,iBAAW,QAAQ;AAAA,QACjB,MAAM;AAAA,QACN,GAAG,oBAAoB,KAAY;AAAA,MACrC,CAAC;AAAA,IACH;AAEA,QAAI,MAAM,MAAM;AACd,WAAK,gBAAgB,OAAO,UAAU;AAAA,IACxC;AAEA,UAAM,QAAQ,+BAAO;AACrB,QAAI,OAAO;AACT,WAAK,aAAa,OAAO,UAAU;AAAA,IACrC;AAAA,EACF;AAAA,EAEQ,gBACN,OACA,YACA;AArIJ;AAsII,SAAK,MAAM,eAAe,sBAAsB,MAAM,WAAW;AACjE,SAAK,MAAM,QAAQ;AAAA,MACjB,aAAa,MAAM,qBAAqB;AAAA,MACxC,eAAc,WAAM,eAAN,YAAoB;AAAA,MAClC,eAAc,WAAM,sBAAN,YAA2B,OAAM,WAAM,eAAN,YAAoB;AAAA,IACrE;AAGA,QAAI,KAAK,MAAM,kBAAkB,CAAC,KAAK,MAAM,WAAW;AACtD,iBAAW,QAAQ,EAAE,MAAM,YAAY,IAAI,KAAK,MAAM,OAAO,CAAC;AAC9D,WAAK,MAAM,YAAY;AAAA,IACzB;AACA,QAAI,KAAK,MAAM,uBAAuB,CAAC,KAAK,MAAM,gBAAgB;AAChE,iBAAW,QAAQ,EAAE,MAAM,iBAAiB,IAAI,IAAI,CAAC;AACrD,WAAK,MAAM,iBAAiB;AAAA,IAC9B;AAAA,EACF;AAAA,EAEQ,aACN,OACA,YACA;AACA,SAAK,mBAAmB,OAAO,UAAU;AACzC,SAAK,gBAAgB,OAAO,UAAU;AACtC,SAAK,iBAAiB,OAAO,UAAU;AAAA,EACzC;AAAA,EAEQ,mBACN,OACA,YACA;AACA,SAAI,+BAAO,YAAW,MAAM;AAC1B,UAAI,CAAC,KAAK,MAAM,gBAAgB;AAC9B,mBAAW,QAAQ,EAAE,MAAM,cAAc,IAAI,KAAK,MAAM,OAAO,CAAC;AAChE,aAAK,MAAM,iBAAiB;AAAA,MAC9B;AACA,iBAAW,QAAQ;AAAA,QACjB,MAAM;AAAA,QACN,IAAI,KAAK,MAAM;AAAA,QACf,OAAO,MAAM;AAAA,MACf,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEQ,gBACN,OACA,YACA;AACA,QAAI,+BAAO,UAAU;AACnB,UAAI,CAAC,KAAK,MAAM,qBAAqB;AACnC,mBAAW,QAAQ,EAAE,MAAM,mBAAmB,IAAI,IAAI,CAAC;AACvD,aAAK,MAAM,sBAAsB;AAAA,MACnC;AACA,iBAAW,QAAQ;AAAA,QACjB,MAAM;AAAA,QACN,IAAI;AAAA,QACJ,OAAO,MAAM;AAAA,MACf,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEQ,iBACN,OACA,YACA;AAtMJ;AAuMI,eAAW,aAAY,WAAM,eAAN,YAAoB,CAAC,GAAG;AAC7C,YAAI,cAAS,aAAT,mBAAmB,SAAQ,MAAM;AACnC,cAAM,IAAI,yBAAyB;AAAA,UACjC,MAAM;AAAA,UACN,SAAS;AAAA,QACX,CAAC;AAAA,MACH;AAEA,YACE,cAAS,aAAT,mBAAmB,SAAQ,UAC3B,cAAS,aAAT,mBAAmB,cAAa,MAChC;AACA,aAAK,aAAa,UAAU,UAAU;AAAA,MACxC;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,aACN,UACA,YACA;AA3NJ;AA4NI,UAAM,MAAK,cAAS,OAAT,aAAgB,sBAAK,QAAO,eAAZ,4CAA8BA,YAAW;AAEpE,eAAW,QAAQ;AAAA,MACjB,MAAM;AAAA,MACN;AAAA,MACA,UAAU,SAAS,SAAS;AAAA,IAC9B,CAAC;AAED,eAAW,QAAQ;AAAA,MACjB,MAAM;AAAA,MACN;AAAA,MACA,OAAO,KAAK,UAAU,SAAS,SAAS,SAAS;AAAA,IACnD,CAAC;AAED,eAAW,QAAQ;AAAA,MACjB,MAAM;AAAA,MACN;AAAA,IACF,CAAC;AAED,eAAW,QAAQ;AAAA,MACjB,MAAM;AAAA,MACN,YAAY;AAAA,MACZ,UAAU,SAAS,SAAS;AAAA,MAC5B,OAAO,KAAK,UAAU,SAAS,SAAS,SAAS;AAAA,IACnD,CAAC;AAED,SAAK,MAAM,eAAe;AAAA,EAC5B;AAAA,EAEQ,eACN,YACA;AAEA,QAAI,KAAK,MAAM,kBAAkB,CAAC,KAAK,MAAM,WAAW;AACtD,iBAAW,QAAQ,EAAE,MAAM,YAAY,IAAI,IAAI,CAAC;AAAA,IAClD;AACA,QAAI,KAAK,MAAM,uBAAuB,CAAC,KAAK,MAAM,gBAAgB;AAChE,iBAAW,QAAQ,EAAE,MAAM,iBAAiB,IAAI,IAAI,CAAC;AAAA,IACvD;AAEA,eAAW,QAAQ;AAAA,MACjB,MAAM;AAAA,MACN,cAAc,KAAK,MAAM;AAAA,MACzB,OAAO,KAAK,MAAM;AAAA,MAClB,kBAAkB;AAAA,QAChB,QAAQ;AAAA,UACN,YAAY,KAAK,MAAM;AAAA,QACzB;AAAA,MACF;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;APxPO,IAAM,+BAAN,MAA8D;AAAA,EAQnE,YAAY,SAA4B,QAAsB;AAP9D,SAAS,uBAAuB;AAchC,SAAS,gBAA0C;AAAA,MACjD,WAAW;AAAA,QACT;AAAA,MACF;AAAA,IACF;AAVE,SAAK,UAAU;AACf,SAAK,SAAS;AACd,SAAK,iBAAiB,IAAI,qBAAqB;AAC/C,SAAK,oBAAoB,IAAI,wBAAwB,MAAM;AAAA,EAC7D;AAAA,EAQA,IAAI,WAAmB;AACrB,WAAO,KAAK,OAAO;AAAA,EACrB;AAAA,EAEA,MAAM,WACJ,SAC6D;AAC7D,UAAM,EAAE,MAAM,MAAM,SAAS,IAAI,MAAM,KAAK,eAAe,OAAO;AAElE,UAAM;AAAA,MACJ;AAAA,MACA,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,IAAI,MAAMC,eAAc;AAAA,MACtB,KAAK,KAAK,OAAO,IAAI;AAAA,QACnB,MAAM;AAAA,QACN,SAAS,KAAK;AAAA,MAChB,CAAC;AAAA,MACD,SAASC,gBAAe,KAAK,OAAO,QAAQ,GAAG,QAAQ,OAAO;AAAA,MAC9D,MAAM,EAAE,GAAG,MAAM,QAAQ,MAAM;AAAA,MAC/B,uBAAuB;AAAA,MACvB,2BAA2BC,2BAA0BC,yBAAwB;AAAA,MAC7E,aAAa,QAAQ;AAAA,MACrB,OAAO,KAAK,OAAO;AAAA,IACrB,CAAC;AAED,UAAM,oBAAoB,KAAK,kBAAkB,wBAAwB,QAAQ;AAEjF,WAAO;AAAA,MACL,GAAG;AAAA,MACH,SAAS,EAAE,MAAM,KAAK,UAAU,IAAI,EAAE;AAAA,MACtC,UAAU;AAAA,QACR,SAAS,KAAK;AAAA,QACd,WAAW,oBAAI,KAAK;AAAA,QACpB,SAAS;AAAA,QACT,MAAM;AAAA,MACR;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,SACJ,SAC2D;AAC3D,UAAM,EAAE,MAAM,MAAM,SAAS,IAAI,MAAM,KAAK,eAAe,OAAO;AAElE,UAAM,EAAE,iBAAiB,OAAO,SAAS,IAAI,MAAMH,eAAc;AAAA,MAC/D,KAAK,KAAK,OAAO,IAAI;AAAA,QACnB,MAAM;AAAA,QACN,SAAS,KAAK;AAAA,MAChB,CAAC;AAAA,MACD,SAASC,gBAAe,KAAK,OAAO,QAAQ,GAAG,QAAQ,OAAO;AAAA,MAC9D,MAAM,EAAE,GAAG,MAAM,QAAQ,KAAK;AAAA,MAC9B,uBAAuB;AAAA,MACvB,2BAA2BG,iCAAgCD,yBAAwB;AAAA,MACnF,aAAa,QAAQ;AAAA,MACrB,OAAO,KAAK,OAAO;AAAA,IACrB,CAAC;AAED,UAAM,kBAAkB,IAAI,sBAAsB,KAAK,MAAM;AAE7D,WAAO;AAAA,MACL,QAAQ,SAAS;AAAA,QACf,gBAAgB,sBAAsB,UAAU,OAAO;AAAA,MACzD;AAAA,MACA,SAAS,EAAE,KAAK;AAAA,MAChB,UAAU,EAAE,SAAS,gBAAgB;AAAA,IACvC;AAAA,EACF;AAAA,EAEA,MAAc,eAAe,SAAuD;AAClF,WAAO,MAAM,KAAK,eAAe,aAAa;AAAA,MAC5C,SAAS,KAAK;AAAA,MACd,GAAG;AAAA,IACL,CAAC;AAAA,EACH;AACF;;;APFO,SAAS,aACd,UAAkC,CAAC,GACnB;AAxHlB;AAyHE,QAAM,WACJ,0BAAqB,QAAQ,OAAO,MAApC,YAAyC;AAE3C,QAAM,gBAAe,aAAQ,SAAR,YAAgB;AAErC,QAAM,aAAa,OAAO;AAAA,IACxB,uBAAuB,QAAQ;AAAA,IAC/B,kBAAkB,QAAQ;AAAA,IAC1B,GAAG,QAAQ;AAAA,EACb;AAEA,QAAM,wBAAwB,CAC5B,SACA,WAAqC,CAAC,MAEtC,IAAI,8BAA8B,SAAS,UAAU;AAAA,IACnD,UAAU,GAAG,YAAY;AAAA,IACzB,KAAK,CAAC,EAAE,KAAK,MAAM,GAAG,OAAO,GAAG,IAAI;AAAA,IACpC,SAAS;AAAA,IACT,OAAO,QAAQ;AAAA,EACjB,CAAC;AAEH,QAAM,uBAAuB,CAC3B,SACA,WAAoC,CAAC,MAErC,IAAI,qBAAqB,SAAS,UAAU;AAAA,IAC1C,UAAU,GAAG,YAAY;AAAA,IACzB,KAAK,CAAC,EAAE,KAAK,MAAM,GAAG,OAAO,GAAG,IAAI;AAAA,IACpC,SAAS;AAAA,IACT,OAAO,QAAQ;AAAA,EACjB,CAAC;AAEH,QAAM,sBAAsB,CAC1B,YAA+B;AAC/B,QAAI,YAAY;AACd,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,WAAO,qBAAqB,OAAO;AAAA,EACrC;AAEA,QAAM,uBAAuB,CAAC,YAA+B;AAC3D,WAAO,IAAI,6BAA6B,SAAS;AAAA,MAC/C,UAAU,GAAG,YAAY;AAAA,MACzB,KAAK,CAAC,EAAE,KAAK,MAAM,GAAG,OAAO,GAAG,IAAI;AAAA,MACpC,SAAS;AAAA,MACT,OAAO,QAAQ;AAAA,IACjB,CAAC;AAAA,EACH;AAEA,QAAM,WAAW,SAAU,SAA4B;AACrD,WAAO,oBAAoB,OAAO;AAAA,EACpC;AAEA,WAAS,gBAAgB;AACzB,WAAS,OAAO;AAChB,WAAS,aAAa;AACtB,WAAS,YAAY;AACrB,WAAS,gBAAgB;AACzB,WAAS,qBAAqB;AAC9B,WAAS,aAAa,CAAC,YAAoB;AACzC,UAAM,IAAI,iBAAiB;AAAA,MACzB;AAAA,MACA,WAAW;AAAA,MACX,SAAS;AAAA,IACX,CAAC;AAAA,EACH;AAEA,SAAO;AACT;AAKO,IAAM,SAAS,aAAa;","names":["z","z","combineHeaders","createJsonResponseHandler","parseProviderOptions","postJsonToApi","z","z","parseProviderOptions","postJsonToApi","combineHeaders","createJsonResponseHandler","combineHeaders","createJsonResponseHandler","createJsonStreamResponseHandler","postJsonToApi","parseProviderOptions","UnsupportedFunctionalityError","UnsupportedFunctionalityError","z","parseProviderOptions","generateId","z","baseOllamaResponseSchema","z","generateId","generateId","generateId","postJsonToApi","combineHeaders","createJsonResponseHandler","baseOllamaResponseSchema","createJsonStreamResponseHandler"]}