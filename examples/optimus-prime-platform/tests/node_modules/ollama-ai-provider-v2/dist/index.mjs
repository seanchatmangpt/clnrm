// src/ollama-provider.ts
import {
  NoSuchModelError
} from "@ai-sdk/provider";
import {
  withoutTrailingSlash
} from "@ai-sdk/provider-utils";

// src/completion/ollama-completion-language-model.ts
import {
  combineHeaders,
  createJsonResponseHandler,
  createJsonStreamResponseHandler,
  parseProviderOptions,
  postJsonToApi
} from "@ai-sdk/provider-utils";
import { z as z2 } from "zod/v4";

// src/completion/ollama-error.ts
import { z } from "zod/v4";
import { createJsonErrorResponseHandler } from "@ai-sdk/provider-utils";
var ollamaErrorDataSchema = z.object({
  error: z.object({
    message: z.string(),
    // The additional information below is handled loosely to support
    // Ollama-compatible providers that have slightly different error
    // responses:
    type: z.string().nullish(),
    param: z.any().nullish(),
    code: z.union([z.string(), z.number()]).nullish()
  })
});
var ollamaFailedResponseHandler = createJsonErrorResponseHandler({
  errorSchema: ollamaErrorDataSchema,
  errorToMessage: (data) => data.error.message
});

// src/adaptors/convert-to-ollama-completion-prompt.ts
import {
  InvalidPromptError,
  UnsupportedFunctionalityError
} from "@ai-sdk/provider";
function convertToOllamaCompletionPrompt({
  prompt,
  user = "user",
  assistant = "assistant"
}) {
  let text = "";
  if (prompt[0].role === "system") {
    text += `${prompt[0].content}

`;
    prompt = prompt.slice(1);
  }
  for (const { role, content } of prompt) {
    switch (role) {
      case "system": {
        throw new InvalidPromptError({
          message: "Unexpected system message in prompt: ${content}",
          prompt
        });
      }
      case "user": {
        const userMessage = content.map((part) => {
          switch (part.type) {
            case "text": {
              return part.text;
            }
          }
        }).filter(Boolean).join("");
        text += `${user}:
${userMessage}

`;
        break;
      }
      case "assistant": {
        const assistantMessage = content.map((part) => {
          switch (part.type) {
            case "text": {
              return part.text;
            }
            case "tool-call": {
              throw new UnsupportedFunctionalityError({
                functionality: "tool-call messages"
              });
            }
          }
        }).join("");
        text += `${assistant}:
${assistantMessage}

`;
        break;
      }
      case "tool": {
        throw new UnsupportedFunctionalityError({
          functionality: "tool messages"
        });
      }
      default: {
        const _exhaustiveCheck = role;
        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);
      }
    }
  }
  text += `${assistant}:
`;
  return {
    prompt: text,
    stopSequences: [`
${user}:`]
  };
}

// src/adaptors/map-ollama-finish-reason.ts
function mapOllamaFinishReason(finishReason) {
  switch (finishReason) {
    case "stop":
      return "stop";
    case "length":
      return "length";
    case "content_filter":
      return "content-filter";
    case "function_call":
    case "tool_calls":
      return "tool-calls";
    default:
      return "unknown";
  }
}

// src/common/get-response-metadata.ts
function getResponseMetadata({
  model,
  created_at
}) {
  return {
    id: void 0,
    modelId: model != null ? model : void 0,
    timestamp: created_at != null ? new Date(created_at) : void 0
  };
}

// src/completion/ollama-completion-language-model.ts
var ollamaCompletionProviderOptions = z2.object({
  think: z2.boolean().optional(),
  user: z2.string().optional(),
  suffix: z2.string().optional(),
  echo: z2.boolean().optional()
});
var OllamaCompletionLanguageModel = class {
  constructor(modelId, settings, config) {
    this.specificationVersion = "v2";
    this.supportedUrls = {
      // No URLs are supported for completion models.
    };
    this.modelId = modelId;
    this.settings = settings;
    this.config = config;
  }
  get provider() {
    return this.config.provider;
  }
  async getArgs({
    prompt,
    maxOutputTokens,
    temperature,
    topP,
    topK,
    frequencyPenalty,
    presencePenalty,
    stopSequences: userStopSequences,
    responseFormat,
    tools,
    toolChoice,
    seed,
    providerOptions
  }) {
    var _a;
    const warnings = [];
    const ollamaOptions = (_a = await parseProviderOptions({
      provider: "ollama",
      providerOptions,
      schema: ollamaCompletionProviderOptions
    })) != null ? _a : {};
    if (topK != null) {
      warnings.push({
        type: "unsupported-setting",
        setting: "topK"
      });
    }
    if (tools == null ? void 0 : tools.length) {
      warnings.push({ type: "unsupported-setting", setting: "tools" });
    }
    if (toolChoice != null) {
      warnings.push({ type: "unsupported-setting", setting: "toolChoice" });
    }
    if (responseFormat != null && responseFormat.type !== "text") {
      warnings.push({
        type: "unsupported-setting",
        setting: "responseFormat",
        details: "JSON response format is not supported."
      });
    }
    const { prompt: completionPrompt, stopSequences } = convertToOllamaCompletionPrompt({ prompt });
    const stop = [...stopSequences != null ? stopSequences : [], ...userStopSequences != null ? userStopSequences : []];
    return {
      args: {
        // model id:
        model: this.modelId,
        // Ollama-supported settings:
        user: ollamaOptions.user,
        think: ollamaOptions.think,
        // standardized settings:
        max_tokens: maxOutputTokens,
        temperature,
        top_p: topP,
        frequency_penalty: frequencyPenalty,
        presence_penalty: presencePenalty,
        stop,
        // prompt:
        prompt: completionPrompt,
        // other settings:
        suffix: ollamaOptions.suffix,
        echo: ollamaOptions.echo,
        stream: false
        // always disabled for doGenerate
      },
      warnings
    };
  }
  async doGenerate(options) {
    var _a, _b, _c, _d;
    const { args: body, warnings } = await this.getArgs(options);
    const {
      responseHeaders,
      value: response,
      rawValue: rawResponse
    } = await postJsonToApi({
      url: this.config.url({
        path: "/generate",
        modelId: this.modelId
      }),
      headers: combineHeaders(this.config.headers(), options.headers),
      body: { ...body, stream: false },
      failedResponseHandler: ollamaFailedResponseHandler,
      successfulResponseHandler: createJsonResponseHandler(
        baseOllamaResponseSchema
      ),
      abortSignal: options.abortSignal,
      fetch: this.config.fetch
    });
    const { prompt: rawPrompt, ...rawSettings } = body;
    const providerMetadata = { ollama: {} };
    return {
      content: [
        {
          type: "text",
          text: response.response
        }
      ],
      usage: {
        inputTokens: (_a = response.prompt_eval_count) != null ? _a : void 0,
        outputTokens: (_b = response.eval_count) != null ? _b : void 0,
        totalTokens: ((_c = response.prompt_eval_count) != null ? _c : 0) + ((_d = response.eval_count) != null ? _d : 0)
      },
      finishReason: mapOllamaFinishReason("stop"),
      request: { body: JSON.stringify(body) },
      response: {
        ...getResponseMetadata(response),
        headers: responseHeaders,
        body: rawResponse
      },
      warnings,
      providerMetadata
    };
  }
  async doStream(options) {
    const { args, warnings } = await this.getArgs(options);
    const body = {
      ...args,
      stream: true
    };
    const { responseHeaders, value: response } = await postJsonToApi({
      url: this.config.url({
        path: "/generate",
        modelId: this.modelId
      }),
      headers: combineHeaders(this.config.headers(), options.headers),
      body,
      failedResponseHandler: ollamaFailedResponseHandler,
      successfulResponseHandler: createJsonStreamResponseHandler(
        baseOllamaResponseSchema
      ),
      abortSignal: options.abortSignal,
      fetch: this.config.fetch
    });
    const { prompt: rawPrompt, ...rawSettings } = args;
    let finishReason = "unknown";
    let usage = {
      inputTokens: void 0,
      outputTokens: void 0,
      totalTokens: void 0
    };
    let isFirstChunk = true;
    return {
      stream: response.pipeThrough(
        new TransformStream({
          transform(chunk, controller) {
            if (!chunk.success) {
              finishReason = "error";
              controller.enqueue({ type: "error", error: chunk.error });
              return;
            }
            const value = chunk.value;
            if ("error" in value) {
              finishReason = "error";
              controller.enqueue({ type: "error", error: value.error });
              return;
            }
            if (isFirstChunk) {
              isFirstChunk = false;
              controller.enqueue({
                type: "response-metadata",
                ...getResponseMetadata(value)
              });
            }
            if (value.done) {
              finishReason = mapOllamaFinishReason("stop");
            }
            if (value.response != null) {
              controller.enqueue({
                type: "text-delta",
                id: "0",
                delta: value.response
              });
            }
          },
          flush(controller) {
            controller.enqueue({
              type: "finish",
              finishReason,
              usage
            });
          }
        })
      ),
      request: { body: JSON.stringify(body) },
      response: { headers: responseHeaders }
    };
  }
};
var baseOllamaResponseSchema = z2.object({
  model: z2.string(),
  created_at: z2.string(),
  response: z2.string(),
  done: z2.boolean(),
  context: z2.array(z2.number()),
  eval_count: z2.number().optional(),
  eval_duration: z2.number().optional(),
  load_duration: z2.number().optional(),
  total_duration: z2.number().optional(),
  prompt_eval_count: z2.number().optional(),
  prompt_eval_duration: z2.number().optional()
});

// src/embedding/ollama-embedding-model.ts
import {
  TooManyEmbeddingValuesForCallError
} from "@ai-sdk/provider";
import {
  combineHeaders as combineHeaders2,
  createJsonResponseHandler as createJsonResponseHandler2,
  parseProviderOptions as parseProviderOptions2,
  postJsonToApi as postJsonToApi2
} from "@ai-sdk/provider-utils";
import { z as z3 } from "zod/v4";
var ollamaEmbeddingProviderOptions = z3.object({
  dimensions: z3.number().optional(),
  truncate: z3.boolean().optional(),
  keepAlive: z3.string().optional()
});
var OllamaEmbeddingModel = class {
  constructor(modelId, settings, config) {
    this.specificationVersion = "v2";
    this.modelId = modelId;
    this.settings = settings;
    this.config = config;
  }
  get provider() {
    return this.config.provider;
  }
  get maxEmbeddingsPerCall() {
    var _a;
    return (_a = this.settings.maxEmbeddingsPerCall) != null ? _a : 2048;
  }
  get supportsParallelCalls() {
    var _a;
    return (_a = this.settings.supportsParallelCalls) != null ? _a : true;
  }
  async getArgs({
    values,
    providerOptions
  }) {
    var _a, _b;
    const ollamaOptions = (_a = await parseProviderOptions2({
      provider: "ollama",
      providerOptions,
      schema: ollamaEmbeddingProviderOptions
    })) != null ? _a : {};
    return {
      args: {
        // model id:
        model: this.modelId,
        input: values,
        // advanced parameters:
        dimensions: (_b = ollamaOptions.dimensions) != null ? _b : this.settings.dimensions,
        truncate: ollamaOptions.truncate,
        keep_alive: ollamaOptions.keepAlive
      }
    };
  }
  async doEmbed({
    values,
    headers,
    abortSignal,
    providerOptions
  }) {
    if (values.length > this.maxEmbeddingsPerCall) {
      throw new TooManyEmbeddingValuesForCallError({
        provider: this.provider,
        modelId: this.modelId,
        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,
        values
      });
    }
    const { args: body } = await this.getArgs({ values, providerOptions });
    const {
      responseHeaders,
      value: response,
      rawValue
    } = await postJsonToApi2({
      url: this.config.url({
        path: "/embed",
        modelId: this.modelId
      }),
      headers: combineHeaders2(this.config.headers(), headers),
      body: { ...body },
      failedResponseHandler: ollamaFailedResponseHandler,
      successfulResponseHandler: createJsonResponseHandler2(
        ollamaTextEmbeddingResponseSchema
      ),
      abortSignal,
      fetch: this.config.fetch
    });
    return {
      embeddings: response.embeddings.map((item) => item),
      usage: { tokens: response.prompt_eval_count },
      response: { headers: responseHeaders, body: rawValue }
    };
  }
};
var ollamaTextEmbeddingResponseSchema = z3.object({
  model: z3.string(),
  embeddings: z3.array(z3.array(z3.number())),
  total_duration: z3.number(),
  load_duration: z3.number(),
  prompt_eval_count: z3.number()
});

// src/responses/ollama-responses-language-model.ts
import {
  combineHeaders as combineHeaders3,
  createJsonResponseHandler as createJsonResponseHandler3,
  createJsonStreamResponseHandler as createJsonStreamResponseHandler2,
  postJsonToApi as postJsonToApi3
} from "@ai-sdk/provider-utils";

// src/responses/ollama-responses-request-builder.ts
import { parseProviderOptions as parseProviderOptions3 } from "@ai-sdk/provider-utils";

// src/responses/convert-to-ollama-responses-messages.ts
import {
  UnsupportedFunctionalityError as UnsupportedFunctionalityError2
} from "@ai-sdk/provider";
function convertToOllamaResponsesMessages({
  prompt,
  systemMessageMode
}) {
  const messages = [];
  const warnings = [];
  for (const { role, content } of prompt) {
    switch (role) {
      case "system": {
        switch (systemMessageMode) {
          case "system": {
            messages.push({ role: "system", content });
            break;
          }
          case "developer": {
            messages.push({ role: "developer", content });
            break;
          }
          case "remove": {
            warnings.push({
              type: "other",
              message: "system messages are removed for this model"
            });
            break;
          }
          default: {
            const _exhaustiveCheck = systemMessageMode;
            throw new Error(
              `Unsupported system message mode: ${_exhaustiveCheck}`
            );
          }
        }
        break;
      }
      case "user": {
        messages.push({
          role: "user",
          content: content.map((part, index) => {
            var _a, _b, _c;
            switch (part.type) {
              case "text": {
                return { type: "input_text", text: part.text };
              }
              case "file": {
                if (part.mediaType.startsWith("image/")) {
                  const mediaType = part.mediaType === "image/*" ? "image/jpeg" : part.mediaType;
                  return {
                    type: "input_image",
                    image_url: part.data instanceof URL ? part.data.toString() : `data:${mediaType};base64,${part.data}`,
                    // Ollama specific extension: image detail
                    detail: (_b = (_a = part.providerOptions) == null ? void 0 : _a.ollama) == null ? void 0 : _b.imageDetail
                  };
                } else if (part.mediaType === "application/pdf") {
                  if (part.data instanceof URL) {
                    throw new UnsupportedFunctionalityError2({
                      functionality: "PDF file parts with URLs"
                    });
                  }
                  return {
                    type: "input_file",
                    filename: (_c = part.filename) != null ? _c : `part-${index}.pdf`,
                    file_data: `data:application/pdf;base64,${part.data}`
                  };
                } else {
                  throw new UnsupportedFunctionalityError2({
                    functionality: `file part media type ${part.mediaType}`
                  });
                }
              }
            }
          })
        });
        break;
      }
      case "assistant": {
        for (const part of content) {
          switch (part.type) {
            case "text": {
              messages.push({
                role: "assistant",
                content: [{ type: "output_text", text: part.text }]
              });
              break;
            }
            case "tool-call": {
              if (part.providerExecuted) {
                break;
              }
              messages.push({
                type: "function_call",
                call_id: part.toolCallId,
                name: part.toolName,
                arguments: JSON.stringify(part.input)
              });
              break;
            }
            case "tool-result": {
              warnings.push({
                type: "other",
                message: `tool result parts in assistant messages are not supported for Ollama responses`
              });
              break;
            }
          }
        }
        break;
      }
      case "tool": {
        for (const part of content) {
          const output = part.output;
          let contentValue;
          switch (output.type) {
            case "text":
            case "error-text":
              contentValue = output.value;
              break;
            case "content":
            case "json":
            case "error-json":
              contentValue = JSON.stringify(output.value);
              break;
          }
          messages.push({
            type: "function_call_output",
            call_id: part.toolCallId,
            output: contentValue
          });
        }
        break;
      }
      default: {
        const _exhaustiveCheck = role;
        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);
      }
    }
  }
  return { messages, warnings };
}

// src/adaptors/convert-to-ollama-chat-messages.ts
function convertToOllamaChatMessages({
  prompt,
  systemMessageMode = "system"
}) {
  const messages = [];
  for (const { role, content } of prompt) {
    switch (role) {
      case "system": {
        switch (systemMessageMode) {
          case "system": {
            messages.push({ role: "system", content });
            break;
          }
          case "developer": {
            messages.push({ role: "developer", content });
            break;
          }
          case "remove": {
            break;
          }
          default: {
            const _exhaustiveCheck = systemMessageMode;
            throw new Error(
              `Unsupported system message mode: ${_exhaustiveCheck}`
            );
          }
        }
        break;
      }
      case "user": {
        if (content.length === 1 && content[0].type === "text") {
          messages.push({ role: "user", content: content[0].text });
          break;
        }
        const userText = content.filter((part) => part.type === "text").map((part) => part.text).join("");
        const images = content.filter((part) => part.type === "file" && part.mediaType.startsWith("image/")).map((part) => part.data);
        messages.push({
          role: "user",
          content: userText.length > 0 ? userText : [],
          images: images.length > 0 ? images : void 0
        });
        break;
      }
      case "assistant": {
        let text = "";
        let thinking = "";
        const toolCalls = [];
        for (const part of content) {
          switch (part.type) {
            case "text": {
              text += part.text;
              break;
            }
            case "tool-call": {
              toolCalls.push({
                id: part.toolCallId,
                type: "function",
                function: {
                  name: part.toolName,
                  arguments: part.input
                }
              });
              break;
            }
            case "reasoning": {
              thinking += part.text;
              break;
            }
            default: {
              throw new Error(`Unsupported part: ${part}`);
            }
          }
        }
        messages.push({
          role: "assistant",
          content: text,
          ...thinking && { thinking },
          tool_calls: toolCalls.length > 0 ? toolCalls : void 0
        });
        break;
      }
      case "tool": {
        for (const toolResponse of content) {
          const output = toolResponse.output;
          let contentValue;
          switch (output.type) {
            case "text":
            case "error-text":
              contentValue = output.value;
              break;
            case "content":
            case "json":
            case "error-json":
              contentValue = JSON.stringify(output.value);
              break;
          }
          messages.push({
            role: "tool",
            tool_call_id: toolResponse.toolCallId,
            content: contentValue
          });
        }
        break;
      }
      default: {
        const _exhaustiveCheck = role;
        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);
      }
    }
  }
  return messages;
}

// src/responses/ollama-responses-prepare-tools.ts
import {
  UnsupportedFunctionalityError as UnsupportedFunctionalityError3
} from "@ai-sdk/provider";
function prepareResponsesTools({
  tools,
  toolChoice
}) {
  tools = (tools == null ? void 0 : tools.length) ? tools : void 0;
  const toolWarnings = [];
  if (tools == null) {
    return { tools: void 0, toolChoice: void 0, toolWarnings };
  }
  const ollamaTools = [];
  for (const tool of tools) {
    switch (tool.type) {
      case "function": {
        let parameters = tool.inputSchema;
        if (!parameters) {
          parameters = {
            type: "object",
            properties: {},
            required: []
          };
        } else if (parameters && typeof parameters === "object" && parameters.type === "object" && parameters.properties && Object.keys(parameters.properties).length === 0) {
          parameters = {
            ...parameters,
            properties: {},
            required: []
          };
        }
        ollamaTools.push({
          type: "function",
          function: {
            name: tool.name,
            description: tool.description,
            parameters
          }
        });
        break;
      }
      default:
        toolWarnings.push({ type: "unsupported-tool", tool });
        break;
    }
  }
  if (toolChoice == null) {
    return { tools: ollamaTools, toolChoice: void 0, toolWarnings };
  }
  const type = toolChoice.type;
  switch (type) {
    case "auto":
    case "none":
    case "required":
      return { tools: ollamaTools, toolChoice: type, toolWarnings };
    case "tool":
      return {
        tools: ollamaTools,
        toolChoice: toolChoice.toolName == "web_search_preview" ? { type: "web_search_preview" } : { type: "function", name: toolChoice.toolName },
        toolWarnings
      };
    default: {
      const _exhaustiveCheck = type;
      throw new UnsupportedFunctionalityError3({
        functionality: `tool choice type: ${_exhaustiveCheck}`
      });
    }
  }
}

// src/ollama-chat-settings.ts
import { z as z4 } from "zod/v4";
var ollamaProviderOptions = z4.object({
  /**
   * Enable or disable the model's thinking process. When enabled, the output will separate
   * the model's thinking from the model's output. When disabled, the model will not think
   * and directly output the content.
   *
   * Only supported by certain models like DeepSeek R1 and Qwen 3.
   */
  think: z4.boolean().optional(),
  options: z4.object({
    num_ctx: z4.number().optional(),
    repeat_last_n: z4.number().optional(),
    repeat_penalty: z4.number().optional(),
    temperature: z4.number().optional(),
    seed: z4.number().optional(),
    stop: z4.array(z4.string()).optional(),
    num_predict: z4.number().optional(),
    top_k: z4.number().optional(),
    top_p: z4.number().optional(),
    min_p: z4.number().optional()
  }).optional()
});

// src/responses/ollama-responses-request-builder.ts
var OllamaRequestBuilder = class {
  async buildRequest({
    modelId,
    maxOutputTokens,
    temperature,
    stopSequences,
    topP,
    topK,
    presencePenalty,
    frequencyPenalty,
    seed,
    prompt,
    providerOptions,
    tools,
    toolChoice,
    responseFormat
  }) {
    const warnings = this.collectUnsupportedSettingsWarnings({
      topK,
      seed,
      presencePenalty,
      frequencyPenalty,
      stopSequences
    });
    const { messages, warnings: messageWarnings } = convertToOllamaResponsesMessages({
      prompt,
      systemMessageMode: "system"
    });
    warnings.push(...messageWarnings);
    const ollamaOptions = await this.parseProviderOptions(providerOptions);
    const baseArgs = this.buildBaseArgs({
      modelId,
      prompt,
      temperature,
      topP,
      maxOutputTokens,
      responseFormat,
      ollamaOptions
    });
    const { tools: ollamaTools, toolChoice: ollamaToolChoice, toolWarnings } = prepareResponsesTools({
      tools,
      toolChoice
    });
    return {
      args: {
        ...baseArgs,
        tools: ollamaTools,
        tool_choice: ollamaToolChoice
      },
      warnings: [...warnings, ...toolWarnings]
    };
  }
  collectUnsupportedSettingsWarnings({
    topK,
    seed,
    presencePenalty,
    frequencyPenalty,
    stopSequences
  }) {
    const warnings = [];
    const unsupportedSettings = [
      { value: topK, name: "topK" },
      { value: seed, name: "seed" },
      { value: presencePenalty, name: "presencePenalty" },
      { value: frequencyPenalty, name: "frequencyPenalty" },
      { value: stopSequences, name: "stopSequences" }
    ];
    for (const { value, name } of unsupportedSettings) {
      if (value != null) {
        warnings.push({ type: "unsupported-setting", setting: name });
      }
    }
    return warnings;
  }
  async parseProviderOptions(providerOptions) {
    const result = await parseProviderOptions3({
      provider: "ollama",
      providerOptions,
      schema: ollamaProviderOptions
    });
    return result != null ? result : null;
  }
  buildBaseArgs({
    modelId,
    prompt,
    temperature,
    topP,
    maxOutputTokens,
    responseFormat,
    ollamaOptions
  }) {
    var _a, _b;
    return {
      model: modelId,
      messages: convertToOllamaChatMessages({
        prompt,
        systemMessageMode: "system"
      }),
      temperature,
      top_p: topP,
      max_output_tokens: maxOutputTokens,
      ...(responseFormat == null ? void 0 : responseFormat.type) === "json" && {
        format: responseFormat.schema != null ? responseFormat.schema : "json"
      },
      think: (_a = ollamaOptions == null ? void 0 : ollamaOptions.think) != null ? _a : false,
      options: (_b = ollamaOptions == null ? void 0 : ollamaOptions.options) != null ? _b : void 0
    };
  }
};

// src/responses/ollama-responses-processor.ts
import { generateId as generateId2 } from "@ai-sdk/provider-utils";
import { z as z5 } from "zod/v4";
var baseOllamaResponseSchema2 = z5.object({
  model: z5.string(),
  created_at: z5.string(),
  done: z5.boolean(),
  message: z5.object({
    content: z5.string(),
    role: z5.string(),
    thinking: z5.string().optional(),
    tool_calls: z5.array(
      z5.object({
        function: z5.object({
          name: z5.string(),
          arguments: z5.record(z5.string(), z5.any())
        }),
        id: z5.string().optional()
      })
    ).optional().nullable()
  }),
  done_reason: z5.string().optional(),
  eval_count: z5.number().optional(),
  eval_duration: z5.number().optional(),
  load_duration: z5.number().optional(),
  prompt_eval_count: z5.number().optional(),
  prompt_eval_duration: z5.number().optional(),
  total_duration: z5.number().optional()
});
var OllamaResponseProcessor = class {
  constructor(config) {
    this.config = config;
  }
  processGenerateResponse(response) {
    const content = this.extractContent(response);
    const finishReason = mapOllamaFinishReason(response.done_reason);
    const usage = this.extractUsage(response);
    const providerMetadata = { ollama: {} };
    return {
      content,
      finishReason,
      usage,
      providerMetadata
    };
  }
  extractContent(response) {
    var _a, _b, _c, _d, _e;
    const content = [];
    const text = response.message.content;
    if (text != null && text.length > 0) {
      content.push({
        type: "text",
        text
      });
    }
    for (const toolCall of (_a = response.message.tool_calls) != null ? _a : []) {
      content.push({
        type: "tool-call",
        toolCallId: (_e = toolCall.id) != null ? _e : (_d = (_c = (_b = this.config).generateId) == null ? void 0 : _c.call(_b)) != null ? _d : generateId2(),
        toolName: toolCall.function.name,
        input: JSON.stringify(toolCall.function.arguments)
      });
    }
    return content;
  }
  extractUsage(response) {
    var _a, _b, _c, _d;
    return {
      inputTokens: (_a = response.prompt_eval_count) != null ? _a : void 0,
      outputTokens: (_b = response.eval_count) != null ? _b : void 0,
      totalTokens: ((_c = response.prompt_eval_count) != null ? _c : 0) + ((_d = response.eval_count) != null ? _d : 0),
      reasoningTokens: void 0,
      // Ollama doesn't provide separate reasoning tokens
      cachedInputTokens: void 0
    };
  }
};
function extractOllamaResponseObjectsFromChunk(chunk) {
  var _a;
  if (chunk.success) {
    return [chunk.value];
  }
  const results = [];
  const raw = (_a = chunk.error) == null ? void 0 : _a.text;
  if (typeof raw !== "string" || raw.length === 0) {
    return results;
  }
  const lines = raw.split(/\r?\n/);
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed === "") continue;
    try {
      const parsed = JSON.parse(trimmed);
      const validated = baseOllamaResponseSchema2.safeParse(parsed);
      if (validated.success) {
        results.push(validated.data);
      }
    } catch (e) {
    }
  }
  return results;
}

// src/responses/ollama-responses-stream-processor.ts
import {
  InvalidResponseDataError
} from "@ai-sdk/provider";
import { generateId as generateId3 } from "@ai-sdk/provider-utils";
var OllamaStreamProcessor = class {
  constructor(config) {
    this.config = config;
    this.state = this.initializeState();
  }
  createTransformStream(warnings, options) {
    return new TransformStream({
      start: (controller) => {
        controller.enqueue({ type: "stream-start", warnings });
      },
      transform: (chunk, controller) => {
        this.processChunk(chunk, controller, options);
      },
      flush: (controller) => {
        this.finalizeStream(controller);
      }
    });
  }
  initializeState() {
    return {
      finishReason: "unknown",
      usage: {
        inputTokens: void 0,
        outputTokens: void 0,
        totalTokens: void 0
      },
      responseId: null,
      ongoingToolCalls: {},
      hasToolCalls: false,
      isFirstChunk: true,
      hasTextStarted: false,
      hasReasoningStarted: false,
      textEnded: false,
      reasoningEnded: false,
      textId: generateId3()
    };
  }
  processChunk(chunk, controller, options) {
    if (options == null ? void 0 : options.includeRawChunks) {
      controller.enqueue({ type: "raw", rawValue: chunk.rawValue });
    }
    const values = extractOllamaResponseObjectsFromChunk(chunk);
    if (values.length === 0) {
      if (!chunk.success) {
        this.state.finishReason = "error";
        controller.enqueue({ type: "error", error: chunk.error });
      }
      return;
    }
    for (const value of values) {
      this.processResponseValue(value, controller);
    }
  }
  processResponseValue(value, controller) {
    if (value && typeof value === "object" && "error" in value) {
      this.state.finishReason = "error";
      controller.enqueue({ type: "error", error: value.error });
      return;
    }
    if (this.state.isFirstChunk) {
      this.state.isFirstChunk = false;
      controller.enqueue({
        type: "response-metadata",
        ...getResponseMetadata(value)
      });
    }
    if (value.done) {
      this.handleDoneChunk(value, controller);
    }
    const delta = value == null ? void 0 : value.message;
    if (delta) {
      this.processDelta(delta, controller);
    }
  }
  handleDoneChunk(value, controller) {
    var _a, _b, _c;
    this.state.finishReason = mapOllamaFinishReason(value.done_reason);
    this.state.usage = {
      inputTokens: value.prompt_eval_count || 0,
      outputTokens: (_a = value.eval_count) != null ? _a : void 0,
      totalTokens: ((_b = value.prompt_eval_count) != null ? _b : 0) + ((_c = value.eval_count) != null ? _c : 0)
    };
    if (this.state.hasTextStarted && !this.state.textEnded) {
      controller.enqueue({ type: "text-end", id: this.state.textId });
      this.state.textEnded = true;
    }
    if (this.state.hasReasoningStarted && !this.state.reasoningEnded) {
      controller.enqueue({ type: "reasoning-end", id: "0" });
      this.state.reasoningEnded = true;
    }
  }
  processDelta(delta, controller) {
    this.processTextContent(delta, controller);
    this.processThinking(delta, controller);
    this.processToolCalls(delta, controller);
  }
  processTextContent(delta, controller) {
    if ((delta == null ? void 0 : delta.content) != null) {
      if (!this.state.hasTextStarted) {
        controller.enqueue({ type: "text-start", id: this.state.textId });
        this.state.hasTextStarted = true;
      }
      controller.enqueue({
        type: "text-delta",
        id: this.state.textId,
        delta: delta.content
      });
    }
  }
  processThinking(delta, controller) {
    if (delta == null ? void 0 : delta.thinking) {
      if (!this.state.hasReasoningStarted) {
        controller.enqueue({ type: "reasoning-start", id: "0" });
        this.state.hasReasoningStarted = true;
      }
      controller.enqueue({
        type: "reasoning-delta",
        id: "0",
        delta: delta.thinking
      });
    }
  }
  processToolCalls(delta, controller) {
    var _a, _b, _c, _d;
    for (const toolCall of (_a = delta.tool_calls) != null ? _a : []) {
      if (((_b = toolCall.function) == null ? void 0 : _b.name) == null) {
        throw new InvalidResponseDataError({
          data: toolCall,
          message: `Expected 'function.name' to be a string.`
        });
      }
      if (((_c = toolCall.function) == null ? void 0 : _c.name) != null && ((_d = toolCall.function) == null ? void 0 : _d.arguments) != null) {
        this.emitToolCall(toolCall, controller);
      }
    }
  }
  emitToolCall(toolCall, controller) {
    var _a, _b, _c, _d;
    const id = (_d = toolCall.id) != null ? _d : (_c = (_b = (_a = this.config).generateId) == null ? void 0 : _b.call(_a)) != null ? _c : generateId3();
    controller.enqueue({
      type: "tool-input-start",
      id,
      toolName: toolCall.function.name
    });
    controller.enqueue({
      type: "tool-input-delta",
      id,
      delta: JSON.stringify(toolCall.function.arguments)
    });
    controller.enqueue({
      type: "tool-input-end",
      id
    });
    controller.enqueue({
      type: "tool-call",
      toolCallId: id,
      toolName: toolCall.function.name,
      input: JSON.stringify(toolCall.function.arguments)
    });
    this.state.hasToolCalls = true;
  }
  finalizeStream(controller) {
    if (this.state.hasTextStarted && !this.state.textEnded) {
      controller.enqueue({ type: "text-end", id: "0" });
    }
    if (this.state.hasReasoningStarted && !this.state.reasoningEnded) {
      controller.enqueue({ type: "reasoning-end", id: "0" });
    }
    controller.enqueue({
      type: "finish",
      finishReason: this.state.finishReason,
      usage: this.state.usage,
      providerMetadata: {
        ollama: {
          responseId: this.state.responseId
        }
      }
    });
  }
};

// src/responses/ollama-responses-language-model.ts
var OllamaResponsesLanguageModel = class {
  constructor(modelId, config) {
    this.specificationVersion = "v2";
    this.supportedUrls = {
      "image/*": [
        /^https?:\/\/.*$/
      ]
    };
    this.modelId = modelId;
    this.config = config;
    this.requestBuilder = new OllamaRequestBuilder();
    this.responseProcessor = new OllamaResponseProcessor(config);
  }
  get provider() {
    return this.config.provider;
  }
  async doGenerate(options) {
    const { args: body, warnings } = await this.prepareRequest(options);
    const {
      responseHeaders,
      value: response,
      rawValue: rawResponse
    } = await postJsonToApi3({
      url: this.config.url({
        path: "/chat",
        modelId: this.modelId
      }),
      headers: combineHeaders3(this.config.headers(), options.headers),
      body: { ...body, stream: false },
      failedResponseHandler: ollamaFailedResponseHandler,
      successfulResponseHandler: createJsonResponseHandler3(baseOllamaResponseSchema2),
      abortSignal: options.abortSignal,
      fetch: this.config.fetch
    });
    const processedResponse = this.responseProcessor.processGenerateResponse(response);
    return {
      ...processedResponse,
      request: { body: JSON.stringify(body) },
      response: {
        modelId: this.modelId,
        timestamp: /* @__PURE__ */ new Date(),
        headers: responseHeaders,
        body: rawResponse
      },
      warnings
    };
  }
  async doStream(options) {
    const { args: body, warnings } = await this.prepareRequest(options);
    const { responseHeaders, value: response } = await postJsonToApi3({
      url: this.config.url({
        path: "/chat",
        modelId: this.modelId
      }),
      headers: combineHeaders3(this.config.headers(), options.headers),
      body: { ...body, stream: true },
      failedResponseHandler: ollamaFailedResponseHandler,
      successfulResponseHandler: createJsonStreamResponseHandler2(baseOllamaResponseSchema2),
      abortSignal: options.abortSignal,
      fetch: this.config.fetch
    });
    const streamProcessor = new OllamaStreamProcessor(this.config);
    return {
      stream: response.pipeThrough(
        streamProcessor.createTransformStream(warnings, options)
      ),
      request: { body },
      response: { headers: responseHeaders }
    };
  }
  async prepareRequest(options) {
    return await this.requestBuilder.buildRequest({
      modelId: this.modelId,
      ...options
    });
  }
};

// src/ollama-provider.ts
function createOllama(options = {}) {
  var _a, _b;
  const baseURL = (_a = withoutTrailingSlash(options.baseURL)) != null ? _a : "http://127.0.0.1:11434/api";
  const providerName = (_b = options.name) != null ? _b : "ollama";
  const getHeaders = () => ({
    "Ollama-Organization": options.organization,
    "Ollama-Project": options.project,
    ...options.headers
  });
  const createCompletionModel = (modelId, settings = {}) => new OllamaCompletionLanguageModel(modelId, settings, {
    provider: `${providerName}.completion`,
    url: ({ path }) => `${baseURL}${path}`,
    headers: getHeaders,
    fetch: options.fetch
  });
  const createEmbeddingModel = (modelId, settings = {}) => new OllamaEmbeddingModel(modelId, settings, {
    provider: `${providerName}.embedding`,
    url: ({ path }) => `${baseURL}${path}`,
    headers: getHeaders,
    fetch: options.fetch
  });
  const createLanguageModel = (modelId) => {
    if (new.target) {
      throw new Error(
        "The Ollama model function cannot be called with the new keyword."
      );
    }
    return createResponsesModel(modelId);
  };
  const createResponsesModel = (modelId) => {
    return new OllamaResponsesLanguageModel(modelId, {
      provider: `${providerName}.responses`,
      url: ({ path }) => `${baseURL}${path}`,
      headers: getHeaders,
      fetch: options.fetch
    });
  };
  const provider = function(modelId) {
    return createLanguageModel(modelId);
  };
  provider.languageModel = createLanguageModel;
  provider.chat = createLanguageModel;
  provider.completion = createCompletionModel;
  provider.embedding = createEmbeddingModel;
  provider.textEmbedding = createEmbeddingModel;
  provider.textEmbeddingModel = createEmbeddingModel;
  provider.imageModel = (modelId) => {
    throw new NoSuchModelError({
      modelId,
      modelType: "imageModel",
      message: "Image generation is unsupported with Ollama"
    });
  };
  return provider;
}
var ollama = createOllama();
export {
  createOllama,
  ollama
};
//# sourceMappingURL=index.mjs.map